{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrwVQsM9TiUw"
      },
      "source": [
        "##### Copyright 2025 Google LLC.\n",
        "\n",
        "Licensed under the Apache 2.0 License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpDUTVKYTowI"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache 2.0 License (the \"License\"); { display-mode: \"form\" }\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtT_DjY6U7hZ"
      },
      "source": [
        "## PH-LLM Professional Exam Evaluation\n",
        "\n",
        "This notebook provides the MCQ evaluation pipeline with dummy data and models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWJsnW4MfoGj"
      },
      "outputs": [],
      "source": [
        "# @title Import\n",
        "import ast\n",
        "import collections\n",
        "import copy\n",
        "import re\n",
        "import time\n",
        "from typing import Any, Callable, List, Optional, Protocol, Set, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAkEoRyf6i5e"
      },
      "outputs": [],
      "source": [
        "# @title MCQ Prompts \u0026 Constants\n",
        "\n",
        "VALID_ANSWER_OPTIONS = [f'({letter})' for letter in 'ABCDEF']\n",
        "\n",
        "MCQS_EVAL_INSTRUCTION = (\n",
        "    'Instruction: The following is a multiple choice question about {domain}'\n",
        "    ' knowledge. Output a single option from {options} as the final answer.\\n'\n",
        ")\n",
        "\n",
        "VALID_HEALTH_DOMAINS = ['Sleep', 'Fitness']\n",
        "\n",
        "SLEEP_COT_MCQ = (\n",
        "    'Instructions: The following are multiple choice questions about {domain}'\n",
        "    ' knowledge. Solve them in a step-by-step fashion, starting by summarizing'\n",
        "    ' the available information. Output a single option from {mcq_options} as'\n",
        "    ' the final answer and enclosed by xml tags \u003canswer\u003e\u003c/answer\u003e.\\n\\n Here are'\n",
        "    ' two examples:\\n ##Question: A 26-year-old female presents asking about'\n",
        "    ' jet lag. She has no past medical history, lives on the East Coast, and'\n",
        "    \" travels frequently to the West Coast for business. The person's career\"\n",
        "    ' involves planning evening events, and she reports significant sleepiness'\n",
        "    ' at these events that impairs her ability to perform her job. She wants to'\n",
        "    ' know how she can adapt to Pacific Standard Time (PST) before she travels.'\n",
        "    ' What treatment plan will help this patient adapt to PST prior to'\n",
        "    ' travel?\\n(A) Light in evening and later bedtime 1 day before'\n",
        "    ' traveling\\n(B) Light in morning and earlier wake time 3 days before'\n",
        "    ' traveling\\n(C) Light in evening and later bedtime 3 days before'\n",
        "    ' traveling\\n(D) Light in morning and earlier wake time 1 month before'\n",
        "    ' traveling\\n(E) Light in evening and later bedtime 1 month before'\n",
        "    \" traveling\\nExplanation: Let's solve this step-by-step, referring to\"\n",
        "    ' authoritative sources as needed. The West Coast is 3 timezones behind the'\n",
        "    ' East Coast. Since she plans evening events, she needs to shift her'\n",
        "    ' schedule to stay up 3 hours later. Adding light in the evening will'\n",
        "    ' disrupt melatonin production, delaying sleepiness. Transitioning'\n",
        "    ' timezones typically takes one day per timezone.\\nAnswer:'\n",
        "    ' \u003canswer\u003e(C)\u003c/answer\u003e\\n\\n##Question: What is a difference in the clinical'\n",
        "    ' features of obstructive sleep apnea (OSA) in older adults compared to'\n",
        "    ' younger adults?\\n(A) Increased prevalence of OSA among older adults'\n",
        "    ' occurs after age 65.\\n(B) Clinical symptoms associated with OSA (e.g.'\n",
        "    ' excessive daytime sleepiness) are less common and less severe in older'\n",
        "    ' adults than in younger adults.\\n(C) Increased risk of cardiopulmonary'\n",
        "    ' diseases is greater among elderly than among younger individuals.\\n(D)'\n",
        "    ' Excess body weight, snoring, and witnessed apneas more consistently'\n",
        "    ' indicate OSA in older adults than in younger individuals.\\n(E) There are'\n",
        "    ' no significant OSA differences between older and younger'\n",
        "    \" adults.\\nExplanation: Let's solve this step-by-step, referring to\"\n",
        "    ' authoritative sources as needed. Compared to younger patients with the'\n",
        "    ' same apnea hypopnea index, OSA in older patients is associated with less'\n",
        "    ' sleepiness (Morrell et al 2012). This observation has led some to suggest'\n",
        "    ' that OSA in the elderly may represent a distinct physiological'\n",
        "    ' phenotype.\\nAnswer: \u003canswer\u003e(B)\u003c/answer\u003e\\n\\n ##Question:'\n",
        "    ' {mcq_question}\\nExplanation: Let us solve this step-by-step, referring to'\n",
        "    ' authoritative sources as needed. '\n",
        ")\n",
        "\n",
        "\n",
        "FITNESS_COT_MCQ = (\n",
        "    'Instructions: The following are multiple choice questions about fitness'\n",
        "    ' knowledge. Solve them in a step-by-step fashion, starting by summarizing'\n",
        "    ' the available information. Output a single option from {mcq_options} as'\n",
        "    ' the final answer and enclosed by xml tags \u003canswer\u003e\u003c/answer\u003e.\\n\\n Here are'\n",
        "    ' two examples: \\n ##Question: A 30-year-old male is looking for a workout'\n",
        "    ' plan to improve his cardiovascular health. He has no known heart'\n",
        "    ' conditions and has a sedentary lifestyle. His goal is to increase stamina'\n",
        "    ' and reduce the risk of heart diseases. Which of the following workout'\n",
        "    ' plans is most suitable for his goals?\\n(A) High-intensity interval'\n",
        "    ' training (HIIT) 5 days a week\\n(B) Moderate-intensity aerobic exercises'\n",
        "    ' like brisk walking 30 minutes a day, 5 days a week\\n(C) Weight training'\n",
        "    ' focused on major muscle groups 4 days a week\\n(D) Yoga and stretching'\n",
        "    ' exercises twice a week\\n(E) Swimming for 60 minutes daily\\nExplanation:'\n",
        "    \" Let's solve this step-by-step, referring to authoritative sources as\"\n",
        "    ' needed. For someone with a sedentary lifestyle looking to improve'\n",
        "    ' cardiovascular health, it is recommended to start with moderate-intensity'\n",
        "    ' aerobic exercises. This approach is effective in increasing stamina and'\n",
        "    ' is less likely to cause injury.\\nAnswer:'\n",
        "    ' \u003canswer\u003e(B)\u003c/answer\u003e\\n\\n##Question: What is a common mistake beginners'\n",
        "    ' make when starting a strength training program?\\n(A) Not including enough'\n",
        "    ' rest days in their routine\\n(B) Focusing only on cardio exercises\\n(C)'\n",
        "    ' Lifting weights that are too heavy leading to poor form\\n(D) Ignoring'\n",
        "    ' flexibility and balance training\\n(E) Spending too much time on warm-up'\n",
        "    \" exercises\\nExplanation: Let's solve this step-by-step, referring to\"\n",
        "    ' authoritative sources as needed. A common mistake for beginners in'\n",
        "    ' strength training is lifting weights that are too heavy, which can lead'\n",
        "    ' to poor form and increase the risk of injury.\\nAnswer:'\n",
        "    ' \u003canswer\u003e(C)\u003c/answer\u003e\\n\\n ##Question: {mcq_question}\\nExplanation: Let us'\n",
        "    ' solve this step-by-step, referring to authoritative sources as needed. '\n",
        ")\n",
        "\n",
        "\n",
        "SLEEP_TAKE_STEP_BACK_MCQ = (\n",
        "    'You are a {domain} expert. I want you to solve a multiple-choice question'\n",
        "    ' in sleep. Here is an example of how to solve a question using an'\n",
        "    ' abstraction-reasoning approach, starting by recalling relevant principles'\n",
        "    ' related to the subject of each question. Then apply these principles step'\n",
        "    ' by step to logically deduce the correct answer. Output a single option'\n",
        "    ' from {mcq_options} as the final answer and enclosed by xml tags'\n",
        "    ' \u003canswer\u003e\u003c/answer\u003e.\\n\\nHere is an example:\\n##Question:\\nA 26-year-old'\n",
        "    ' female presents asking about jet lag. She has no past medical history,'\n",
        "    ' lives on the East Coast, and travels frequently to the West Coast for'\n",
        "    ' business. Her career involves planning evening events, and she reports'\n",
        "    ' significant sleepiness at these events that impairs her ability to'\n",
        "    ' perform her job. She wants to know how she can adapt to Pacific Standard'\n",
        "    ' Time (PST) before she travels. What treatment plan will help this patient'\n",
        "    ' adapt to PST prior to travel?\\nOptions:\\n(A) Light in evening and later'\n",
        "    ' bedtime 1 day before traveling\\n(B) Light in morning and earlier wake'\n",
        "    ' time 3 days before traveling\\n(C) Light in evening and later bedtime 3'\n",
        "    ' days before traveling\\n(D) Light in morning and earlier wake time 1 month'\n",
        "    ' before traveling\\n(E) Light in evening and later bedtime 1 month before'\n",
        "    ' traveling\\n\\n## Principles:\\nCircadian Rhythms: The human body operates'\n",
        "    ' on a circadian rhythm, an internal clock that cycles roughly every 24'\n",
        "    ' hours. This rhythm is influenced by external cues, especially light.'\n",
        "    ' Exposure to light can shift the circadian rhythm, making a person feel'\n",
        "    \" more awake or sleepy.\\n\\nJet Lag: Jet lag occurs when a person's internal\"\n",
        "    ' clock is out of sync with the time zone they are in. This is common when'\n",
        "    ' traveling across multiple time zones. To adjust to a new time zone, the'\n",
        "    \" body's circadian rhythm needs to be shifted.\\n\\nLight Therapy: Exposure\"\n",
        "    ' to light at certain times can help shift the circadian rhythm. Exposure'\n",
        "    ' to light in the morning advances the circadian clock (making one wake up'\n",
        "    ' earlier), while exposure in the evening delays it (making one stay awake'\n",
        "    ' later).\\n\\n## Answer:\\nUsing the principles of Circadian Rhythms, Jet'\n",
        "    ' Lag, and Light Therapy, we can solve the problem as following:\\nThe'\n",
        "    ' patient needs to adapt from the East Coast time to the West Coast time,'\n",
        "    ' which is 3 hours behind. To do this, she needs to adjust her body clock'\n",
        "    ' to wake up and go to sleep later according to her current (East Coast)'\n",
        "    ' time zone, which aligns with the normal waking and sleeping hours in the'\n",
        "    ' Pacific Standard Time.\\n\\nLooking at the options:\\n- (A) and (C) suggest'\n",
        "    ' delaying the circadian rhythm (light in the evening and later bedtime),'\n",
        "    ' which would make her wake up and sleep later according to East Coast'\n",
        "    ' time. However, this is counterproductive as it would exacerbate the issue'\n",
        "    ' when she is on the West Coast.\\n- (B) Light in the morning and earlier'\n",
        "    ' wake time 3 days before traveling would advance her circadian rhythm.'\n",
        "    ' This means she would wake up earlier according to East Coast time, which'\n",
        "    ' is aligned with waking up at a regular time in PST.\\n- (D) and (E)'\n",
        "    ' propose changes starting 1 month before traveling, which is impractical'\n",
        "    ' for someone who travels frequently.\\n\\nTherefore, the correct answer is'\n",
        "    ' \u003canswer\u003e(B)\u003c/answer\u003e. Light in the morning and earlier wake time 3 days'\n",
        "    ' before traveling. This method would advance her circadian rhythm to'\n",
        "    ' better match the Pacific Standard Time, helping her to cope with her'\n",
        "    ' sleepiness during evening events on the West Coast.\\n\\n ## Question:\\n'\n",
        "    ' {mcq_question}\\n\\n## Principles:\\n'\n",
        ")\n",
        "\n",
        "\n",
        "FITNESS_TAKE_STEP_BACK_MCQ = (\n",
        "    'You are a fitness expert. I want you to solve a multiple-choice question'\n",
        "    ' in fitness. Here is an example of how to solve a question using an'\n",
        "    ' abstraction-reasoning approach, starting by recalling relevant principles'\n",
        "    ' related to the subject of each question. Then apply these principles step'\n",
        "    ' by step to logically deduce the correct answer. Output a single option'\n",
        "    ' from {mcq_options} as the final answer and enclosed by xml tags'\n",
        "    ' \u003canswer\u003e\u003c/answer\u003e.\\n\\nHere is an example: \\n##Question:\\nA 35-year-old'\n",
        "    ' male is looking to increase muscle mass. He has been working out'\n",
        "    ' consistently for a year and follows a balanced diet. He wants to know'\n",
        "    ' which change in his workout routine will be most effective for gaining'\n",
        "    ' muscle mass. What would you recommend?\\nOptions:\\n(A) Increase cardio'\n",
        "    ' exercises and decrease weight lifting\\n(B) Focus on high-repetition'\n",
        "    ' weight lifting with lower weights\\n(C) Incorporate high-intensity'\n",
        "    ' interval training (HIIT) twice a week\\n(D) Increase weight lifting with'\n",
        "    ' heavier weights and lower repetitions\\n(E) Maintain the current routine'\n",
        "    ' without changes\\n\\n## Principles:\\nMuscle Hypertrophy: Muscle growth'\n",
        "    ' occurs when muscle fibers are damaged and repair themselves, leading to'\n",
        "    ' an increase in muscle size. This is best achieved through resistance'\n",
        "    ' training that challenges the muscles.\\n\\nProgressive Overload: To'\n",
        "    \" continue gaining muscle, it's important to progressively increase the\"\n",
        "    ' demands on the musculoskeletal system. This can be done by lifting'\n",
        "    ' heavier weights, increasing repetitions, or changing the exercises'\n",
        "    ' performed.\\n\\nExercise Variation: Incorporating a variety of exercises'\n",
        "    ' can help target different muscle groups and prevent plateaus in muscle'\n",
        "    ' growth.\\n\\n## Answer:\\nUsing the principles of Muscle Hypertrophy,'\n",
        "    ' Progressive Overload, and Exercise Variation, we can solve the problem'\n",
        "    ' as following:\\nThe individual is already engaged in consistent workouts'\n",
        "    ' and has a balanced diet, which is fundamental for muscle growth. To'\n",
        "    ' further enhance muscle mass, the focus should be on increasing the'\n",
        "    ' intensity of workouts in a way that challenges the muscles more'\n",
        "    ' significantly.\\n\\nLooking at the options:\\n- (A) focuses on increasing'\n",
        "    ' cardio, which is less effective for muscle hypertrophy compared to'\n",
        "    ' resistance training.\\n- (B) involves high-repetition lifting with lower'\n",
        "    ' weights, which is more endurance-focused rather than hypertrophy.\\n- (C)'\n",
        "    ' HIIT can be beneficial for overall fitness but is not the most efficient'\n",
        "    ' for muscle growth compared to targeted resistance training.\\n- (D)'\n",
        "    ' Increasing weight lifting with heavier weights and lower repetitions is'\n",
        "    ' aligned with the principles of muscle hypertrophy and progressive'\n",
        "    ' overload.\\n- (E) Maintaining the current routine will not provide the'\n",
        "    ' necessary stimulus for further muscle growth.\\n\\nTherefore, the correct'\n",
        "    ' answer is \u003canswer\u003e(D)\u003c/answer\u003e. Increasing weight lifting with heavier'\n",
        "    ' weights and lower repetitions will effectively promote muscle growth by'\n",
        "    ' adhering to the principles of muscle hypertrophy and progressive'\n",
        "    ' overload.\\n\\n ## Question:\\n {mcq_question}\\n\\n## Principles:\\n'\n",
        ")\n",
        "\n",
        "SLEEP_MCQ_INPUTS_FEATURE_NAME = 'question'\n",
        "SLEEP_MCQ_TARGETS_FEATURE_NAME = 'answer'\n",
        "SLEEP_MCQ_DIFFICULTY_FEATURE_NAME = 'difficulty'\n",
        "SLEEP_MCQ_EVAL_LABELS_FEATURE_NAME = 'choices'\n",
        "SLEEP_MCQ_DIFF_LEVEL_EASY = 'Easy'\n",
        "SLEEP_MCQ_DIFF_LEVEL_MODERATE = 'Moderate'\n",
        "SLEEP_MCQ_DIFF_LEVEL_HARD = 'Hard'\n",
        "FITNESS_MCQ_INPUTS_FEATURE_NAME = 'question'\n",
        "FITNESS_MCQ_TARGETS_FEATURE_NAME = 'answer'\n",
        "FITNESS_MCQ_EVAL_LABELS_FEATURE_NAME = 'choices'\n",
        "\n",
        "\n",
        "def create_prompt_to_generate_mcqs(\n",
        "    mcq_question: str, mcq_options: dict[str, str], mcq_domain: str\n",
        ") -\u003e str:\n",
        "  \"\"\"Converts a MCQ example to a prompt.\"\"\"\n",
        "  if not mcq_question or not mcq_options or not mcq_domain:\n",
        "    raise ValueError('MCQ example is missing required fields.')\n",
        "  if mcq_domain not in VALID_HEALTH_DOMAINS:\n",
        "    raise ValueError(f'MCQ domain {mcq_domain} is not supported.')\n",
        "  if set(mcq_options) != set(VALID_ANSWER_OPTIONS[: len(mcq_options)]):\n",
        "    raise ValueError(f'MCQ options are not valid: {mcq_options}')\n",
        "  instruction = MCQS_EVAL_INSTRUCTION.format(\n",
        "      domain=mcq_domain, options=', '.join(sorted(mcq_options))\n",
        "  )\n",
        "  prompt = instruction + mcq_question\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr3ENUgQu-EB"
      },
      "outputs": [],
      "source": [
        "# @title Define Your Own LLM\n",
        "\n",
        "\n",
        "class LanguageModel(Protocol):\n",
        "  \"\"\"Protocol defining the expected interface for a language model object.\"\"\"\n",
        "\n",
        "  def Generate(self, prompt: str, **kwargs) -\u003e List[str]:\n",
        "    \"\"\"Generates outputs based on a given prompt.\n",
        "\n",
        "    Args:\n",
        "      prompt: The input text prompt.\n",
        "      **kwargs: Additional arguments for generation (e.g., max_tokens,\n",
        "        temperature).\n",
        "\n",
        "    Returns:\n",
        "      A list of generated text outputs.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def Score(self, prompt: str, completion: str, **kwargs) -\u003e float:\n",
        "    \"\"\"Returns the total log probability score of a 'completion' string\n",
        "\n",
        "    given a 'prompt' string.\n",
        "\n",
        "    Args:\n",
        "      prompt: The input text prompt (e.g., the question).\n",
        "      completion: The text whose log probability is to be computed when appended\n",
        "        to the prompt (e.g., the expected answer).\n",
        "      **kwargs: Additional arguments for scoring.\n",
        "\n",
        "    Returns:\n",
        "      A float representing the total log probability of the completion\n",
        "      given the prompt. A higher (less negative) value indicates higher\n",
        "      likelihood.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "\n",
        "# --- Example Concrete Implementation of a Language Model ---\n",
        "class MyExampleLanguageModel:\n",
        "  \"\"\"An example concrete implementation of a language model.\n",
        "\n",
        "  In a real scenario, this would wrap your actual LM (e.g., an OpenAI API\n",
        "  client, a Hugging Face model, a custom local model).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model_identifier: str, num_connections: int = 1, **kwargs):\n",
        "    print(\n",
        "        f\"Initializing MyExampleLanguageModel with: {model_identifier=},\"\n",
        "        f\" {num_connections=}, {kwargs=}\"\n",
        "    )\n",
        "    self._model_identifier = model_identifier\n",
        "    self._num_connections = num_connections\n",
        "    # In a real implementation, you would load your model here\n",
        "    # e.g., self._model = load_from_huggingface(model_identifier)\n",
        "    # Or self._api_client = apiclient(api_key=kwargs.get(\"api_key\"))\n",
        "    self._config = kwargs\n",
        "\n",
        "  def Generate(\n",
        "      self,\n",
        "      prompt: str,\n",
        "  ) -\u003e List[str]:\n",
        "    \"\"\"Example implementation for text generation.\"\"\"\n",
        "    return f\"Your prompt: {prompt} was processed.\"\n",
        "\n",
        "  def Score(self, prompt: str, completion: str) -\u003e float:\n",
        "    \"\"\"Example implementation for scoring text (returning log probability of completion given prompt).\"\"\"\n",
        "    print(f\"Scoring completion '{completion}' given prompt '{prompt}'\")\n",
        "\n",
        "    sampled_prob = np.random.rand()\n",
        "    dummy_log_prob = np.log(sampled_prob)\n",
        "\n",
        "    return dummy_log_prob\n",
        "\n",
        "\n",
        "# --- Modified _get_lm_model function ---\n",
        "def _get_lm_model(\n",
        "    model_address: str,\n",
        "    num_conn: int = 1,\n",
        "    **kwargs,\n",
        ") -\u003e (\n",
        "    LanguageModel\n",
        "):  # Type hint that it returns an object conforming to LanguageModel\n",
        "  \"\"\"Get an LM model.\n",
        "\n",
        "  This is a stub method for users to implement their own logic to load\n",
        "  any language model (LM).\n",
        "\n",
        "  Args:\n",
        "    model_address: The address or identifier for the model to load.\n",
        "    num_conn: The number of parallel calls to use when running the model.\n",
        "    **kwargs: Additional keyword arguments that can be passed to the underlying\n",
        "      model loading mechanism.\n",
        "\n",
        "  Returns:\n",
        "    An instance of the loaded language model, conforming to the LanguageModel\n",
        "    interface (i.e., having Generate and Score methods).\n",
        "  \"\"\"\n",
        "  return MyExampleLanguageModel(model_address, num_conn, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJExEAd6ZUP"
      },
      "outputs": [],
      "source": [
        "def find_majority_vote_answer(\n",
        "    dict_list: List[Tuple[str, Set[str]]],\n",
        ") -\u003e Optional[Tuple[str, Set[str]]]:\n",
        "  \"\"\"Finds the tuple with the majority vote from a list of tuples.\"\"\"\n",
        "  # Extract identifiers and count them\n",
        "  identifiers = []\n",
        "  for entry in dict_list:\n",
        "    entry_answer = entry['model_answer']\n",
        "    if entry_answer not in ['(A)', '(B)', '(C)', '(D)', '(E)']:\n",
        "      continue\n",
        "    else:\n",
        "      identifiers.append(entry_answer)\n",
        "  counts = collections.Counter(identifiers)\n",
        "  most_common = counts.most_common(2)\n",
        "  # Check for a clear majority\n",
        "  if len(most_common) == 1 or (\n",
        "      len(most_common) \u003e 1 and most_common[0][1] \u003e most_common[1][1]\n",
        "  ):\n",
        "    majority_identifier = most_common[0][0]\n",
        "    # Find and return the first tuple with the majority identifier\n",
        "    for t in dict_list:\n",
        "      if t['model_answer'] == majority_identifier:\n",
        "        return (t['model_answer'], t['model_generations'])\n",
        "  # If no majority or a tie, return None\n",
        "  return None\n",
        "\n",
        "\n",
        "def _postprocess_generation_answer(generations: set[str]) -\u003e str:\n",
        "  \"\"\"Process the generated answers.\"\"\"\n",
        "  answer_re = re.compile(r'\u003canswer\u003e(\\([ABCDE]\\))\u003c/answer\u003e', re.IGNORECASE)\n",
        "  answers = []\n",
        "  for gen in generations:\n",
        "    gen = gen.strip()\n",
        "    matcher = answer_re.search(gen)\n",
        "    if matcher:\n",
        "      answers.append(matcher.group(1).upper())\n",
        "  # If no generation yielded a valid formatted Answer, flag as skipped.\n",
        "  if not answers:\n",
        "    return {_MODEL_GEN: generations, _SKIPPED: 1}\n",
        "  # This extracts a list of most common answers within the xml tags\n",
        "  # \u003canswer\u003e\u003c/answer\u003e, takes the first entry (in case of ties), and then\n",
        "  # extracts the answer text (the second entry in the pair is the number of\n",
        "  # times it appeared).\n",
        "  model_answer = collections.Counter(answers).most_common(1)[0][0]\n",
        "  return model_answer\n",
        "\n",
        "\n",
        "def add_instruction_to_prompt(\n",
        "    samples: list[dict[str, Any]], domain: str\n",
        ") -\u003e list[dict[str, Any]]:\n",
        "  \"\"\"Returns samples with `inputs` modified to add instruction to prompt.\"\"\"\n",
        "  if domain == 'Sleep':\n",
        "    input_key = SLEEP_MCQ_INPUTS_FEATURE_NAME\n",
        "    choices_key = SLEEP_MCQ_EVAL_LABELS_FEATURE_NAME\n",
        "  elif domain == 'Fitness':\n",
        "    input_key = FITNESS_MCQ_INPUTS_FEATURE_NAME\n",
        "    choices_key = FITNESS_MCQ_EVAL_LABELS_FEATURE_NAME\n",
        "  else:\n",
        "    raise ValueError(f'Invalid domain: {domain}')\n",
        "\n",
        "  retval = []\n",
        "  for orig_sample in samples:\n",
        "    sample = copy.deepcopy(orig_sample)\n",
        "    sample[input_key] = create_prompt_to_generate_mcqs(\n",
        "        sample[input_key], sample[choices_key], domain\n",
        "    )\n",
        "    retval.append(sample)\n",
        "  return retval\n",
        "\n",
        "\n",
        "def read_mcq_dataset(\n",
        "    dataset_path: str, domain: str, difficulty_level: Optional[list[str]] = None\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Reads the MCQ dataset.\"\"\"\n",
        "  with open(dataset_path, 'r') as f:\n",
        "    synthetic_mcq_dataset = pd.read_csv(f)\n",
        "\n",
        "  synthetic_mcq_dataset['choices'] = synthetic_mcq_dataset['choices'].apply(\n",
        "      ast.literal_eval\n",
        "  )\n",
        "\n",
        "  if difficulty_level:\n",
        "    synthetic_mcq_dataset = synthetic_mcq_dataset[\n",
        "        (synthetic_mcq_dataset['domain'] == domain)\n",
        "        \u0026 (synthetic_mcq_dataset['difficulty'].isin(difficulty_level))\n",
        "    ]\n",
        "  else:\n",
        "    synthetic_mcq_dataset = synthetic_mcq_dataset[\n",
        "        synthetic_mcq_dataset['domain'] == domain\n",
        "    ]\n",
        "  return synthetic_mcq_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpQ_DRcwAbXA"
      },
      "outputs": [],
      "source": [
        "# @title Model Evaluation\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    llm_address: str,\n",
        "    dataset_path: str,\n",
        "    domain: str,\n",
        "    eval_func: Callable[\n",
        "        [\n",
        "            Any,\n",
        "            str,\n",
        "            dict[str, str],\n",
        "            Optional[str],\n",
        "            Optional[float],\n",
        "            Optional[int],\n",
        "            Optional[int],\n",
        "        ],\n",
        "        dict[str, int],\n",
        "    ],\n",
        "    num_examples: int = -1,\n",
        "    num_replicas: int = 1,\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    sc_round: int = 1,\n",
        "    mcq_difficulty_level: Optional[list[str]] = None,\n",
        ") -\u003e list[dict[str, Any]]:\n",
        "  \"\"\"Returns counts of 'correct', 'incorrect', 'skipped' questions.\n",
        "\n",
        "  Args:\n",
        "    llm_address: Path to the LLM address.\n",
        "    dataset_path: Path to the dataset of MCQ example questions.\n",
        "    domain: The domain of the MCQ dataset (e.g. 'sleep' or 'fitness').\n",
        "    eval_func: Function used to evaluate the model specified at `llm_address`.\n",
        "    num_examples: Number of examples to evaluate. If \u003c0, evaluates all examples.\n",
        "    num_replicas: Number of model replicas available. To parallelize, we need to\n",
        "      both specify the number of connections to open to the server and then run\n",
        "      parallel evaluations on the model.\n",
        "    prompt_type: The type of prompt to use (e.g., CoT or Step-Back).\n",
        "    temperature: The temperature to use for the llm model.\n",
        "    max_decoding_steps: The maximum number of decoding steps to run.\n",
        "    sc_round: The round of self-consistency.\n",
        "    mcq_difficulty_level: The difficulty level of the MCQ dataset.\n",
        "\n",
        "  Returns:\n",
        "    A list of the examples featurized as dictionaries along with the model\n",
        "    results.\n",
        "  \"\"\"\n",
        "  start_time = time.time()\n",
        "  model = _get_lm_model(llm_address, num_conn=num_replicas)\n",
        "  if domain == 'Sleep':\n",
        "    feature_dicts = read_mcq_dataset(\n",
        "        dataset_path,\n",
        "        domain=domain,\n",
        "        difficulty_level=mcq_difficulty_level,\n",
        "    )\n",
        "  elif domain == 'Fitness':\n",
        "    feature_dicts = read_mcq_dataset(\n",
        "        dataset_path, domain=domain, difficulty_level='None'\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(f'Unknown domain: {domain}')\n",
        "  if num_examples \u003c 0:\n",
        "    num_examples = len(feature_dicts)\n",
        "  examples_to_evaluate = feature_dicts[:num_examples]\n",
        "  def _run_one_example(feats: dict[str, Any]) -\u003e dict[str, Any]:\n",
        "    inputs = feats.copy()\n",
        "    res = eval_func(\n",
        "        model=model,\n",
        "        domain=domain,\n",
        "        features=inputs,\n",
        "        prompt_type=prompt_type,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        sc_round=sc_round,\n",
        "    )\n",
        "    assert set(res.keys()).isdisjoint(set(inputs.keys()))\n",
        "    res.update(inputs)\n",
        "    return res\n",
        "\n",
        "  retval = []\n",
        "  for _, ex in examples_to_evaluate.iterrows():\n",
        "    ex = _run_one_example(ex.to_dict())\n",
        "    retval.append(ex)\n",
        "  print(\n",
        "      f'Evaluated {dataset_path} with {llm_address} in'\n",
        "      f' {time.time() - start_time} seconds using {num_replicas} workers.',\n",
        "      flush=True,\n",
        "  )\n",
        "  return retval\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Methods for evaluating MCQs.\n",
        "################################################################################\n",
        "\n",
        "# Potential outcomes from evaluating the model on the question.\n",
        "_CORRECT = 'correct'\n",
        "_INCORRECT = 'incorrect'\n",
        "_SKIPPED = 'skipped'\n",
        "_NO_MAJORITY_VOTE = {'NO MAJORITY VOTE, USED lm.Score INSTEAD'}\n",
        "\n",
        "# The answer the model provided (if not _SKIPPED).\n",
        "_MODEL_ANSWER = 'model_answer'\n",
        "\n",
        "# Relevant only for lm.Score -- the raw logprobs of each choice.\n",
        "_MODEL_SCORES = 'model_scores'\n",
        "\n",
        "# Relevant only for lm.Generate -- the generated text.\n",
        "_MODEL_GEN = 'model_generations'\n",
        "\n",
        "\n",
        "def eval_score(\n",
        "    *,\n",
        "    model: Any,\n",
        "    domain: str,\n",
        "    features: dict[str, Any],\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 5,\n",
        "    sc_round=None,\n",
        ") -\u003e dict[str, Any]:\n",
        "  \"\"\"Returns correct/incorrect for the question when evaluated with lm.Score.\"\"\"\n",
        "  del sc_round  # unused.\n",
        "  del prompt_type  # unused.\n",
        "  del temperature  # unused.\n",
        "  del max_decoding_steps  # unused.\n",
        "  full_question = add_instruction_to_prompt([features], domain=domain)[0][\n",
        "      'question'\n",
        "  ]\n",
        "\n",
        "  # Run lm.Score for the question.\n",
        "  scores = []\n",
        "  for ao in features['choices']:\n",
        "    scores.extend(model.Score(full_question, ao))\n",
        "  model_answer = list(features['choices'].keys())[np.argmax(scores)]\n",
        "  return {\n",
        "      _MODEL_ANSWER: model_answer,\n",
        "      _MODEL_SCORES: scores,\n",
        "      _CORRECT if model_answer == features['answer'] else _INCORRECT: 1,\n",
        "  }\n",
        "\n",
        "\n",
        "def _create_mcq_generate_prompt(\n",
        "    mcq_question: str,\n",
        "    mcq_options: dict[str, str],\n",
        "    prompt_type: str,\n",
        "    domain: str,\n",
        ") -\u003e str:\n",
        "  \"\"\"Converts a sleep MCQ question to a generate prompt.\"\"\"\n",
        "  if prompt_type == 'step_back' and domain == 'Sleep':\n",
        "    return SLEEP_TAKE_STEP_BACK_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  elif prompt_type == 'cot' and domain == 'Sleep':\n",
        "    return SLEEP_COT_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  elif prompt_type == 'cot' and domain == 'Fitness':\n",
        "    return FITNESS_COT_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  elif prompt_type == 'step_back' and domain == 'Fitness':\n",
        "    return FITNESS_TAKE_STEP_BACK_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        f'Unsupported combination of prompt type and domain: {prompt_type} and'\n",
        "        f' {domain=}.'\n",
        "    )\n",
        "\n",
        "\n",
        "def eval_generate(\n",
        "    *,\n",
        "    model: Any,\n",
        "    domain: str,\n",
        "    features: dict[str, str],\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    sc_round=None,\n",
        ") -\u003e dict[str, Any]:\n",
        "  \"\"\"Returns correct/incorrect for the question when evaluated with lm.Generate.\"\"\"\n",
        "  del sc_round  # unused.\n",
        "  full_question = _create_mcq_generate_prompt(\n",
        "      features['question'],\n",
        "      features['choices'],\n",
        "      prompt_type,\n",
        "      domain,\n",
        "  )\n",
        "  generations = {gen for gen in model.Generate(full_question)}\n",
        "  model_answer = _postprocess_generation_answer(generations)\n",
        "  return {\n",
        "      _MODEL_ANSWER: model_answer,\n",
        "      _CORRECT if model_answer == features['answer'] else _INCORRECT: 1,\n",
        "      _MODEL_GEN: generations,\n",
        "  }\n",
        "\n",
        "\n",
        "def eval_generate_sc(\n",
        "    *,\n",
        "    model: Any,\n",
        "    domain: str,\n",
        "    features: dict[str, str],\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    sc_round: int = 5,\n",
        ") -\u003e dict[str, Any]:\n",
        "  \"\"\"Returns correct/incorrect for the question when evaluated with lm.Generate.\"\"\"\n",
        "  sc_generations = [\n",
        "      eval_generate(\n",
        "          model=model,\n",
        "          domain=domain,\n",
        "          features=features,\n",
        "          prompt_type=prompt_type,\n",
        "          temperature=temperature,\n",
        "          max_decoding_steps=max_decoding_steps,\n",
        "          sc_round=None,\n",
        "      )\n",
        "      for _ in range(sc_round)\n",
        "  ]\n",
        "  most_popular_answer = find_majority_vote_answer(sc_generations)\n",
        "  if not most_popular_answer:\n",
        "    retval = eval_score(\n",
        "        model=model,\n",
        "        domain=domain,\n",
        "        features=features,\n",
        "        prompt_type=None,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        sc_round=sc_round,\n",
        "    )\n",
        "    del retval[_MODEL_SCORES]\n",
        "    retval[_MODEL_GEN] = _NO_MAJORITY_VOTE\n",
        "    return retval\n",
        "  else:\n",
        "    model_answer, generations = most_popular_answer\n",
        "  return {\n",
        "      _MODEL_ANSWER: model_answer,\n",
        "      _CORRECT if model_answer == features['answer'] else _INCORRECT: 1,\n",
        "      _MODEL_GEN: generations,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaydEMlcobwM"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "005JRqOfPaXl"
      },
      "outputs": [],
      "source": [
        "def _accuracy(results: list[dict[str, Any]]) -\u003e tuple[int, int, float]:\n",
        "  \"\"\"Returns (correct, incorrect, accuracy) tuple.\"\"\"\n",
        "  correct = sum(q.get(_CORRECT, 0) for q in results)\n",
        "  incorrect = sum(q.get(_INCORRECT, 0) for q in results)\n",
        "  acc = np.nan if correct + incorrect == 0 else correct / (correct + incorrect)\n",
        "  return correct, incorrect, acc\n",
        "\n",
        "\n",
        "def analyze_results(results: list[dict[str, Any]]) -\u003e None:\n",
        "  \"\"\"Prints out analysis of results, both stratified and combined.\"\"\"\n",
        "  stratifications = {'All': results}\n",
        "  num_questions_with_difficulty = sum(int('difficulty' in q) for q in results)\n",
        "  if num_questions_with_difficulty not in [0, len(results)]:\n",
        "    raise ValueError(\n",
        "        'Expected either all or none of the questions to be annotated with '\n",
        "        f'difficulty, found {num_questions_with_difficulty}/{len(results)}.'\n",
        "    )\n",
        "  if num_questions_with_difficulty:\n",
        "    for difficulty in {q['difficulty'] for q in results}:\n",
        "      stratifications[difficulty] = [\n",
        "          q for q in results if q['difficulty'] == difficulty\n",
        "      ]\n",
        "\n",
        "  for diff, strat in sorted(stratifications.items()):\n",
        "    correct, incorrect, acc = _accuracy(strat)\n",
        "    print(\n",
        "        f'Accuracy for {diff} questions: {correct}/{correct + incorrect} ='\n",
        "        f' {acc:.2f}'\n",
        "    )\n",
        "\n",
        "\n",
        "def save_results(results: list[dict[str, Any]], filename: str) -\u003e None:\n",
        "  \"\"\"Saves results to a CSV file.\"\"\"\n",
        "  df_results = pd.DataFrame(results)\n",
        "  with open(filename, 'w') as f:\n",
        "    df_results.to_csv(f, index=True)\n",
        "\n",
        "\n",
        "def perform_full_evaluation(\n",
        "    *,\n",
        "    llm_address: str,\n",
        "    dataset_path: str,\n",
        "    domain: str,\n",
        "    outroot: str | None = None,\n",
        "    num_examples: int = -1,\n",
        "    num_replicas: int = 1,\n",
        "    prompt_type: Optional[str] = None,\n",
        "    use_eval_generate: bool = False,\n",
        "    temperature: float = 0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    mcq_difficulty_level: Optional[list[str]] = None,\n",
        "    sc_round: Optional[int] = None,\n",
        ") -\u003e None:\n",
        "  \"\"\"Performs full evaluation.\"\"\"\n",
        "  if outroot:\n",
        "    outroot += f'.{llm_address.split(\"/\")[-1]}'\n",
        "\n",
        "  if not use_eval_generate:\n",
        "    print('Using model.Score for evaluation.')\n",
        "    score_test_results = evaluate_model(\n",
        "        llm_address=llm_address,\n",
        "        dataset_path=dataset_path,\n",
        "        domain=domain,\n",
        "        eval_func=eval_score,\n",
        "        num_examples=num_examples,\n",
        "        num_replicas=num_replicas,\n",
        "        mcq_difficulty_level=mcq_difficulty_level,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    print('## Results for lm.Score evaluation. ##')\n",
        "    print('\\n# Test data:')\n",
        "    analyze_results(score_test_results)\n",
        "    if outroot:\n",
        "      score_test_save_path = outroot.format(split='test') + '.score.csv'\n",
        "      save_results(score_test_results, score_test_save_path)\n",
        "      return score_test_results\n",
        "  elif use_eval_generate and sc_round:\n",
        "    print('Using model.Generate with self-consistency for evaluation.')\n",
        "    generate_test_results = evaluate_model(\n",
        "        llm_address=llm_address,\n",
        "        dataset_path=dataset_path,\n",
        "        domain=domain,\n",
        "        eval_func=eval_generate_sc,\n",
        "        num_examples=num_examples,\n",
        "        num_replicas=num_replicas,\n",
        "        prompt_type=prompt_type,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        sc_round=sc_round,\n",
        "        mcq_difficulty_level=mcq_difficulty_level,\n",
        "    )\n",
        "    print(f'## Results for self-consistency {prompt_type} evaluation. ##')\n",
        "    print('\\n# Test data:')\n",
        "    analyze_results(generate_test_results)\n",
        "    if outroot:\n",
        "      generate_test_save_path = (\n",
        "          outroot.format(split='test') + f'.{prompt_type}.sc.csv'\n",
        "      )\n",
        "      save_results(generate_test_results, generate_test_save_path)\n",
        "    return generate_test_results\n",
        "  else:\n",
        "    print('Using model.Generate for evaluation.')\n",
        "    generate_test_results = evaluate_model(\n",
        "        llm_address=llm_address,\n",
        "        dataset_path=dataset_path,\n",
        "        domain=domain,\n",
        "        eval_func=eval_generate,\n",
        "        num_examples=num_examples,\n",
        "        num_replicas=num_replicas,\n",
        "        prompt_type=prompt_type,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        mcq_difficulty_level=mcq_difficulty_level,\n",
        "    )\n",
        "    print(f'## Results for lm.Generate {prompt_type} evaluation. ##')\n",
        "    print('\\n# Test data:')\n",
        "    analyze_results(generate_test_results)\n",
        "    if outroot:\n",
        "      generate_test_save_path = (\n",
        "          outroot.format(split='test') + f'.{prompt_type}.csv'\n",
        "      )\n",
        "      save_results(generate_test_results, generate_test_save_path)\n",
        "    return generate_test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTI_z2vv_ZwU"
      },
      "outputs": [],
      "source": [
        "# @title Generate Synthetic Dummy MCQs\n",
        "\n",
        "import json\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        'question': 'What is the recommended amount of sleep for adults?',\n",
        "        'answer': '(C)',\n",
        "        'choices': collections.OrderedDict([\n",
        "            ('(A)', '4-5 hours'),\n",
        "            ('(B)', '6-7 hours'),\n",
        "            ('(C)', '7-9 hours'),\n",
        "            ('(D)', '10-12 hours'),\n",
        "        ]),\n",
        "        'domain': 'Sleep',\n",
        "        'difficulty': 'Easy',\n",
        "    },\n",
        "    {\n",
        "        'question': (\n",
        "            'Which stage of sleep is characterized by rapid eye movements and'\n",
        "            ' vivid dreams?'\n",
        "        ),\n",
        "        'answer': '(D)',\n",
        "        'choices': collections.OrderedDict([\n",
        "            ('(A)', 'Stage 1'),\n",
        "            ('(B)', 'Stage 2'),\n",
        "            ('(C)', 'NREM'),\n",
        "            ('(D)', 'REM'),\n",
        "        ]),\n",
        "        'domain': 'Sleep',\n",
        "        'difficulty': 'Moderate',\n",
        "    },\n",
        "    {\n",
        "        'question': (\n",
        "            'A patient presents with excessive daytime sleepiness and loud'\n",
        "            ' snoring. What is a likely diagnosis?'\n",
        "        ),\n",
        "        'answer': '(B)',\n",
        "        'choices': collections.OrderedDict([\n",
        "            ('(A)', 'Insomnia'),\n",
        "            ('(B)', 'Sleep Apnea'),\n",
        "            ('(C)', 'Narcolepsy'),\n",
        "            ('(D)', 'Restless Legs Syndrome'),\n",
        "        ]),\n",
        "        'domain': 'Sleep',\n",
        "        'difficulty': 'Hard',\n",
        "    },\n",
        "    {\n",
        "        'question': (\n",
        "            'What hormone is primarily responsible for regulating the'\n",
        "            ' sleep-wake cycle?'\n",
        "        ),\n",
        "        'answer': '(A)',\n",
        "        'choices': collections.OrderedDict([\n",
        "            ('(A)', 'Melatonin'),\n",
        "            ('(B)', 'Cortisol'),\n",
        "            ('(C)', 'Insulin'),\n",
        "            ('(D)', 'Adrenaline'),\n",
        "        ]),\n",
        "        'domain': 'Sleep',\n",
        "        'difficulty': 'Easy',\n",
        "    },\n",
        "    {\n",
        "        'question': 'Which of the following is a common symptom of insomnia?',\n",
        "        'answer': '(C)',\n",
        "        'choices': collections.OrderedDict([\n",
        "            ('(A)', 'Loud snoring'),\n",
        "            ('(B)', 'Daytime alertness'),\n",
        "            ('(C)', 'Difficulty falling or staying asleep'),\n",
        "            ('(D)', 'Sudden sleep attacks'),\n",
        "        ]),\n",
        "        'domain': 'Sleep',\n",
        "        'difficulty': 'Moderate',\n",
        "    },\n",
        "    {\n",
        "        'question': (\n",
        "            'Cognitive Behavioral Therapy for Insomnia (CBT-I) typically'\n",
        "            ' includes which of the following components?'\n",
        "        ),\n",
        "        'answer': '(E)',\n",
        "        'choices': collections.OrderedDict([\n",
        "            ('(A)', 'Only medication prescription'),\n",
        "            ('(B)', 'Strict diet restrictions'),\n",
        "            ('(C)', 'Exposure therapy to fears'),\n",
        "            ('(D)', 'Biofeedback alone'),\n",
        "            ('(E)', 'Sleep restriction and stimulus control'),\n",
        "        ]),\n",
        "        'domain': 'Sleep',\n",
        "        'difficulty': 'Hard',\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert the 'choices' OrderedDict to a standard dictionary and then to a JSON string\n",
        "# json.dumps is generally safer and more standard for serializing dictionaries\n",
        "# for eventual parsing, even if ast.literal_eval is used on the reading side.\n",
        "df['choices'] = df['choices'].apply(lambda x: json.dumps(dict(x)))\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('./synthetic_mcq_data.csv', index=False)\n",
        "\n",
        "print(\n",
        "    \"Dummy 'synthetic_mcq_data.csv' has been created successfully with\"\n",
        "    ' corrected choices format.'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXqmSD0hLRj"
      },
      "outputs": [],
      "source": [
        "llm_addresses = ['dummy_llm_address']\n",
        "g_num_model_replicas = 5  # @param {type:\"integer\"}\n",
        "g_domain = 'Sleep'  # @param ['Sleep', 'Fitness']\n",
        "temperature = 0.7  # @param {type:\"number\"}\n",
        "max_decoding_steps = 2048  # @param {type:\"integer\"}\n",
        "max_decoding_steps_score = 5\n",
        "sc_round = 3  # @param {type:\"integer\"}\n",
        "dataset_path = './synthetic_mcq_data.csv'  # @param\n",
        "outroot = '/tmp/'  # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo9fjQ8j2rVv"
      },
      "source": [
        "## CoT + Self Consistency - LM.Generate / Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdCloL-Ix4E-"
      },
      "outputs": [],
      "source": [
        "for llm_address in llm_addresses:\n",
        "  _ = perform_full_evaluation(\n",
        "      llm_address=llm_address,\n",
        "      dataset_path=dataset_path,\n",
        "      domain=g_domain,  # or 'Fitness'\n",
        "      outroot=outroot,\n",
        "      # num_examples=3, # Only used for debugging.\n",
        "      num_replicas=g_num_model_replicas,\n",
        "      prompt_type='cot',\n",
        "      use_eval_generate=True,\n",
        "      temperature=temperature,\n",
        "      max_decoding_steps=max_decoding_steps,\n",
        "      mcq_difficulty_level=[\n",
        "          SLEEP_MCQ_DIFF_LEVEL_EASY,\n",
        "          SLEEP_MCQ_DIFF_LEVEL_MODERATE,\n",
        "          SLEEP_MCQ_DIFF_LEVEL_HARD,\n",
        "      ],\n",
        "      sc_round=None,\n",
        "  )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/genomics/phllm_oss/professional_exams/PHLLM_MCQ_Eval.ipynb",
          "timestamp": 1749656321366
        },
        {
          "file_id": "phllm/colabs/Sleep_MCQ_Evaluation.ipynb",
          "timestamp": 1721846119161
        },
        {
          "file_id": "17TV61f-oec2TgJ3noJjSw2p8qQQ8EZBa",
          "timestamp": 1705883012466
        },
        {
          "file_id": "1LQzE6liyyIlWaimZY65OB-5sRFmGpFU3",
          "timestamp": 1705878004644
        },
        {
          "file_id": "phllm/colabs/Sleep_MCQ_Evaluation.ipynb",
          "timestamp": 1705789529171
        },
        {
          "file_id": "1ZB6QbSF2zkdsP_fYNUELLB5zB3Xya7q-",
          "timestamp": 1704217291474
        },
        {
          "file_id": "phllm/colabs/Sleep_MCQ_Evaluation.ipynb",
          "timestamp": 1701923968069
        },
        {
          "file_id": "phllm/colabs/[Benchmark]Sleep_MCQ_Evaluation_w_SAX.ipynb",
          "timestamp": 1699909204266
        },
        {
          "file_id": "phllm/colabs/[Benchmark]Sleep_MCQ_Evaluation_w_SAX.ipynb",
          "timestamp": 1699241447723
        },
        {
          "file_id": "1Lp3O8BxktAm5RNTThoHxV05kKOZN1L8x",
          "timestamp": 1698970731249
        },
        {
          "file_id": "14MsmHAGNYMQnbcmJ8e4AlapMy_OGrSwk",
          "timestamp": 1698943122736
        },
        {
          "file_id": "1Plg3CmxXliA9N7nuHZxaehJKt4mSo8dj",
          "timestamp": 1698880037422
        },
        {
          "file_id": "12d2rX1smVQb8Hj44CPxY8872WVs2RBVX",
          "timestamp": 1698858038991
        },
        {
          "file_id": "1b8etzPVJ5O_C-oCqkpwAncSfPzvF7aZ6",
          "timestamp": 1698771181969
        },
        {
          "file_id": "1qydk6_5J16gvv_qZgvwGgJjGFoC07uB6",
          "timestamp": 1697045490482
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
