{
  "cells": [
    {
      "metadata": {
        "id": "buvo0YEccJ77"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "F42ePhOqUlc-"
      },
      "cell_type": "markdown",
      "source": [
        "# PH-LLM figures\n",
        "\n",
        "TODO(jtcosentino): Flush out README, overall documentation, and function doc\n",
        "strings.\n",
        "\n",
        "This notebook reproduces a subset of the main and extended data figures from the\n",
        "PH-LLM manuscript. Specifically:\n",
        "\n",
        "-   Fig. 2.{c,b}: Long-form case study evaluation and performance.\n",
        "-   Extended Data Fig. 3: Pairwise Gwet's AC2 measuring inter-rater reliability\n",
        "    between primary and secondary raters.\n",
        "-   Extended Data Fig. 4: Contingency tables showing pairwise rating agreement\n",
        "    between raters.\n",
        "-   Extended Data Fig. 5: Sleep and fitness case study human evaluation results\n",
        "    by principle.\n",
        "-   Extended Data Fig. 6: Contingency tables showing pairwise rating agreement\n",
        "    between our best AutoRaters, their corresponding expert raters, and other\n",
        "    experts.\n",
        "-   Extended Data Fig. 7: Automatic evaluation of coaching recommendations\n",
        "    across PH-LLM and baseline models.\n",
        "-   Extended Data Fig. 8: Effect of fine-tuning data scale on model performance\n",
        "    in coaching recommendations.\n",
        "\n",
        "This notebooks assumes that it will be run on\n",
        "[Google Colab](https://colab.research.google.com/) and requires that the\n",
        "following files be uploaded to the `/content/` directory:\n",
        "\n",
        "-   `fitness_autoeval_external_ratings.tsv`: Fitness AutoEval ratings for\n",
        "    external model comparisons (i.e., comparing PH-LLM, GPT 4 Turbo, Claude 3\n",
        "    Opus, etc.).\n",
        "-   `fitness_autoeval_subsample_ratings.tsv`: Fitness AutoEval ratings for\n",
        "    subsampled model comparisons (i.e., comparing Gemini Ultra, PH-LLM trained\n",
        "    with 25% of the training dataset, PH-LLM trained with 50% of the training\n",
        "    dataset, and PH-LLM).\n",
        "-   `fitness_human_expert_ratings.tsv`: Fitness human expert ratings.\n",
        "-   `sleep_autoeval_external_ratings.tsv`: Sleep AutoEval ratings for external\n",
        "    model comparisons (i.e., comparing PH-LLM, GPT 4 Turbo, Claude 3 Opus,\n",
        "    etc.).\n",
        "-   `sleep_autoeval_subsample_ratings.tsv`: Sleep AutoEval ratings for\n",
        "    subsampled model comparisons (i.e., comparing Gemini Ultra, PH-LLM trained\n",
        "    with 25% of the training dataset, PH-LLM trained with 50% of the training\n",
        "    dataset, and PH-LLM).\n",
        "-   `sleep_human_expert_ratings.tsv`: Sleep human expert ratings.\n",
        "-   `pro_preds_bootstrap.tsv`: Bootstrapping results of model predictions for\n",
        "    Patient Reported Outcomes (PRO).\n",
        "-   `pro_prevalence.tsv`: Prevalence of binary targets in Patient Reported\n",
        "    Outcomes (PRO).\n",
        "\n",
        "Output figures are also written to the same `/content/` directory as PDFs:\n",
        "\n",
        "-   `figure_2_cd.pdf`\n",
        "-   `figure_3_cd.pdf`\n",
        "-   `extended_data_figure_3.pdf`\n",
        "-   `extended_data_figure_4.pdf`\n",
        "-   `extended_data_figure_5.pdf`\n",
        "-   `extended_data_figure_6.pdf`\n",
        "-   `extended_data_figure_7.pdf`\n",
        "-   `extended_data_figure_8.pdf`\n",
        "-   `extended_data_figure_9.pdf`"
      ]
    },
    {
      "metadata": {
        "id": "l4TXlaOSEh01"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependencies and prepare environment"
      ]
    },
    {
      "metadata": {
        "id": "tDdtLo-SEg4o"
      },
      "cell_type": "code",
      "source": [
        "!pip install irrCAC\n",
        "\n",
        "# Google Colab comes with NumPy preinstalled. However, installing irrCAC also\n",
        "# installs a newer version of NumPy that is required for irrCAC. Thus we need to\n",
        "# restart the runtime for the changes to take effect.\n",
        "# After the session is restarted, you can run the rest of the notebook.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "m8pMYsKG4Pbe"
      },
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import collections\n",
        "import concurrent.futures\n",
        "import dataclasses\n",
        "import enum\n",
        "import itertools\n",
        "import os\n",
        "import string\n",
        "from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence\n",
        "\n",
        "import immutabledict\n",
        "import irrCAC.raw\n",
        "from matplotlib import container\n",
        "from matplotlib import figure\n",
        "from matplotlib import text\n",
        "from matplotlib import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "import seaborn as sns\n",
        "import sklearn.metrics\n",
        "import statsmodels.stats.multitest"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8-H2wFRoImaP"
      },
      "cell_type": "code",
      "source": [
        "# Set global plotting styles.\n",
        "sns.set_style('white')\n",
        "plt.rc('axes.spines', top=False, right=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "rp-mLkjKndJ0"
      },
      "cell_type": "markdown",
      "source": [
        "### Global constants"
      ]
    },
    {
      "metadata": {
        "id": "dOlbZHw-4e7A"
      },
      "cell_type": "code",
      "source": [
        "class RaterType(enum.Enum):\n",
        "  \"\"\"Denotes whether a rater is a primary or secondary rater.\"\"\"\n",
        "\n",
        "  PRIMARY = 'primary'\n",
        "  SECONDARY = 'secondary'\n",
        "\n",
        "\n",
        "class Vertical(enum.Enum):\n",
        "  \"\"\"Denotes a vertical.\"\"\"\n",
        "\n",
        "  FITNESS = 'fitness'\n",
        "  SLEEP = 'sleep'\n",
        "\n",
        "\n",
        "class RatingsSource(enum.Enum):\n",
        "  \"\"\"Denotes a source of ratings.\"\"\"\n",
        "\n",
        "  HUMAN_EXPERT = 'human_expert'\n",
        "  AUTOEVAL_SUBSAMPLE = 'autoeval_subsample'\n",
        "  AUTOEVAL_EXTERNAL = 'autoeval_external'\n",
        "\n",
        "\n",
        "# A mapping of vertical to ratings base dir.\n",
        "_VERTICAL_TO_BASE_DIR = immutabledict.immutabledict({\n",
        "    Vertical.FITNESS: '/content/',\n",
        "    Vertical.SLEEP: '/content/',\n",
        "})\n",
        "\n",
        "# Ratings dataframe columns used when plotting.\n",
        "COL_RATING: str = 'rating'\n",
        "COL_RATING_HUMAN_EXPERT: str = 'rating_human_expert'\n",
        "COL_RATING_AUTOEVAL_FITNESS: str = 'rating_autoeval_fitness_primary_c_high_var'\n",
        "COL_RATING_AUTOEVAL_SLEEP: str = 'rating_autoeval_sleep_primary_c_high_var'\n",
        "COL_CASE_STUDY_ID: str = 'case_study_id'\n",
        "COL_CONVERSATION_SOURCE: str = 'conversation_source'\n",
        "COL_SECTION_TAG: str = 'tag'\n",
        "COL_PRINCIPLE: str = 'principle'\n",
        "COL_RATER: str = 'rater'\n",
        "\n",
        "# Columns derived from the dataframe and used within plotting function.\n",
        "DERIVED_COL_KEY: str = 'key'\n",
        "DERIVED_COL_KEY_NO_RATER: str = 'key_no_rater'\n",
        "\n",
        "# Conversation source keys (i.e., the source of a given response).\n",
        "KEY_HUMAN_EXPERT: str = 'human_expert'\n",
        "KEY_GEMINI_ULTRA: str = 'gemini_ultra'\n",
        "KEY_PHLLM: str = 'phllm'\n",
        "KEY_GPT_4_TURBO: str = 'gpt_4_turbo'\n",
        "KEY_CLAUDE_3_OPUS: str = 'claude_3_opus'\n",
        "KEY_SUBSAMPLE_25: str = 'phllm_subsample_25pct'\n",
        "KEY_SUBSAMPLE_50: str = 'phllm_subsample_50pct'\n",
        "\n",
        "# Color hex codes used when plotting.\n",
        "COLOR_BLUE: str = '#4285f4'\n",
        "COLOR_BLUE_LIGHT: str = '#D3E3FD'\n",
        "COLOR_BLUE_DARK: str = '#076EFF'\n",
        "COLOR_BLUE_MID: str = '#4FABFF'\n",
        "COLOR_GRAY = '#34495E'\n",
        "COLOR_GRAY_DARK = '#202124'\n",
        "COLOR_GREEN: str = '#1E8E3E'\n",
        "COLOR_ORANGE: str = '#FFB482'\n",
        "COLOR_PINK_LIGHT: str = '#FFC7E8'\n",
        "COLOR_PINK_DARK: str = '#FBA9D6'\n",
        "COLOR_PURPLE_DARK: str = '#C58AF9'\n",
        "COLOR_YELLOW: str = '#FBBC04'\n",
        "\n",
        "# A mapping of conversation source keys to figure labels.\n",
        "CONVERSATION_SOURCE_KEY_TO_LABEL: immutabledict.immutabledict[str, str] = (\n",
        "    immutabledict.immutabledict({\n",
        "        KEY_HUMAN_EXPERT: 'Human Expert',\n",
        "        KEY_GEMINI_ULTRA: 'Gemini Ultra',\n",
        "        KEY_PHLLM: 'PH-LLM',\n",
        "        KEY_GPT_4_TURBO: 'GPT-4 Turbo',\n",
        "        KEY_CLAUDE_3_OPUS: 'Claude 3 Opus',\n",
        "        KEY_SUBSAMPLE_25: 'Subsample 25%',\n",
        "        KEY_SUBSAMPLE_50: 'Subsample 50%',\n",
        "    })\n",
        ")\n",
        "\n",
        "# A mapping of AutoEval model IDs to figure labels.\n",
        "AUTOEVAL_MODEL_TO_LABEL: immutabledict.immutabledict[str, str] = (\n",
        "    immutabledict.immutabledict({\n",
        "        COL_RATING_AUTOEVAL_FITNESS: 'Best AutoRater (C)',\n",
        "        COL_RATING_AUTOEVAL_SLEEP: 'Best AutoRater (C)',\n",
        "    })\n",
        ")\n",
        "\n",
        "# A mapping of conversation source keys to source colors.\n",
        "CONVERSATION_SOURCE_PALETTE: immutabledict.immutabledict[str, str] = (\n",
        "    immutabledict.immutabledict({\n",
        "        KEY_HUMAN_EXPERT: COLOR_BLUE_LIGHT,\n",
        "        KEY_GEMINI_ULTRA: COLOR_BLUE_MID,\n",
        "        KEY_PHLLM: COLOR_BLUE_DARK,\n",
        "        KEY_GPT_4_TURBO: COLOR_ORANGE,\n",
        "        KEY_CLAUDE_3_OPUS: COLOR_PINK_LIGHT,\n",
        "        KEY_SUBSAMPLE_25: COLOR_PINK_DARK,\n",
        "        KEY_SUBSAMPLE_50: COLOR_PURPLE_DARK,\n",
        "    })\n",
        ")\n",
        "\n",
        "\n",
        "# A tuple defining the order of conversation sources in the main figure.\n",
        "CONVERSATION_SOURCE_ORDER_MAIN: tuple[str, ...] = (\n",
        "    KEY_GEMINI_ULTRA,\n",
        "    KEY_PHLLM,\n",
        "    KEY_HUMAN_EXPERT,\n",
        ")\n",
        "\n",
        "# A tuple defining the order of conversation sources in the external figure.\n",
        "CONVERSATION_SOURCE_ORDER_EXTERNAL: tuple[str, ...] = (\n",
        "    *CONVERSATION_SOURCE_ORDER_MAIN,\n",
        "    KEY_GPT_4_TURBO,\n",
        "    KEY_CLAUDE_3_OPUS,\n",
        ")\n",
        "\n",
        "# A tuple defining the order of conversation sources in the subsample figure.\n",
        "CONVERSATION_SOURCE_ORDER_SUBSAMPLE: tuple[str, ...] = (\n",
        "    KEY_GEMINI_ULTRA,\n",
        "    KEY_SUBSAMPLE_25,\n",
        "    KEY_SUBSAMPLE_50,\n",
        "    KEY_PHLLM,\n",
        ")\n",
        "\n",
        "# A tuple defining the order of fitness section tags in figures.\n",
        "FITNESS_SECTION_TAG_ORDER: tuple[str, ...] = (\n",
        "    'training_load',\n",
        "    'sleep',\n",
        "    'health_metrics',\n",
        "    'assessment',\n",
        ")\n",
        "\n",
        "# A tuple defining the order of sleep section tags in figures.\n",
        "SLEEP_SECTION_TAG_ORDER: tuple[str, ...] = (\n",
        "    'insights',\n",
        "    'etiology',\n",
        "    'recommendations',\n",
        ")\n",
        "\n",
        "# A mapping of section tags to figure labels.\n",
        "SECTION_TAG_TO_LABEL: immutabledict.immutabledict[str, str] = (\n",
        "    immutabledict.immutabledict({\n",
        "        'training_load': 'Training Load',\n",
        "        'sleep': 'Sleep',\n",
        "        'health_metrics': 'Health Metrics',\n",
        "        'assessment': 'Assessment',\n",
        "        'insights': 'Insights',\n",
        "        'etiology': 'Etiology',\n",
        "        'recommendations': 'Recommendations',\n",
        "    })\n",
        ")\n",
        "\n",
        "# A tuple defining the order of principles in figures.\n",
        "PRINCIPLE_ORDER: tuple[str, ...] = (\n",
        "    'important_user_data',\n",
        "    'no_unimportant_user_data',\n",
        "    'no_incorrect_user_data',\n",
        "    'important_interpretations',\n",
        "    'no_unimportant_interpretations',\n",
        "    'no_incorrect_important_interpretations',\n",
        "    'no_incorrect_unimportant_interpretations',\n",
        "    'no_assumptions',\n",
        "    'important_domain_knowledge',\n",
        "    'no_unimportant_domain_knowledge',\n",
        "    'no_incorrect_domain_knowledge',\n",
        "    'no_hallucinations',\n",
        "    'non_harmful',\n",
        "    'readable',\n",
        "    'overall_quality',\n",
        ")\n",
        "\n",
        "# A mapping of principles to figure labels.\n",
        "PRINCIPLE_TO_LABEL: immutabledict.immutabledict[str, str] = (\n",
        "    immutabledict.immutabledict({\n",
        "        'important_user_data': 'Important User Data',\n",
        "        'no_unimportant_user_data': 'No Unimportant User Data',\n",
        "        'no_incorrect_user_data': 'No Incorrect User Data',\n",
        "        'important_interpretations': 'Important Interpretations',\n",
        "        'no_unimportant_interpretations': 'No Unimportant Interpretations',\n",
        "        'no_incorrect_important_interpretations': (\n",
        "            'No Incorrect Important Interpretations'\n",
        "        ),\n",
        "        'no_incorrect_unimportant_interpretations': (\n",
        "            'No Incorrect Unimportant Interpretations'\n",
        "        ),\n",
        "        'no_assumptions': 'No Assumptions',\n",
        "        'important_domain_knowledge': 'Important Domain Knowledge',\n",
        "        'no_unimportant_domain_knowledge': 'No Unimportant Domain Knowledge',\n",
        "        'no_incorrect_domain_knowledge': 'No Incorrect Domain Knowledge',\n",
        "        'no_hallucinations': 'No Hallucinations',\n",
        "        'non_harmful': 'Non Harmful',\n",
        "        'readable': 'Readable',\n",
        "        'overall_quality': 'Overall Quality',\n",
        "    })\n",
        ")\n",
        "\n",
        "# A mapping of metrics to figure labels.\n",
        "METRIC_TO_LABEL_BINARY = immutabledict.immutabledict({\n",
        "    'gwet_ac2': \"Gwet's AC2\",\n",
        "})\n",
        "\n",
        "# General labels used in plotting (e.g., axes).\n",
        "LABEL_SECTION: str = 'Section'\n",
        "LABEL_PRINCIPLE: str = 'Principle'\n",
        "LABEL_AVG_RATING: str = 'Average Rating'\n",
        "\n",
        "\n",
        "# Constants for Patient Reported Constants (PRO).\n",
        "\n",
        "# Path to bootstrapping results of PRO predictions and prevalence of labels.\n",
        "PRO_BOOTSTRAP_PATH = '/content/pro_preds_bootstrap.tsv'\n",
        "PRO_PREVALENCE_PATH = '/content/pro_prevalence.tsv'\n",
        "\n",
        "# Mapping from binary targets in PRO to their names shown in figure.\n",
        "PRO_BINARY_TARGETS_TO_NAME = {\n",
        "    'alertness_score_binary': 'Alert',\n",
        "    'tiredness_score_binary': 'Tiredness',\n",
        "    'satisfied_score_binary': 'Satisfied',\n",
        "    'refreshing_score_binary': 'Refreshed',\n",
        "    'sleepy_during_daytime_score_binary': 'Sleepy during daytime',\n",
        "    'restless_score_binary': 'Very restless',\n",
        "    'trouble_falling_asleep_score_binary': 'Trouble falling asleep',\n",
        "    'enough_sleep_score_binary': 'Enough sleep',\n",
        "    'trouble_staying_asleep_score_binary': 'Trouble staying asleep',\n",
        "    'trouble_concentrating_score_sleepimpairment_binary': (\n",
        "        'SI due to trouble concentrating'\n",
        "    ),\n",
        "    'trouble_sleeping_score_binary': 'Trouble sleeping',\n",
        "    'irritability_score_sleepimpairment_binary': 'SI due to irritability',\n",
        "    'trouble_productivity_score_binary': 'Trouble being productive',\n",
        "    'problems_score_binary': 'Having problems',\n",
        "    'quality_score_binary': 'Quality',\n",
        "    'trouble_staying_awake_score_binary': 'Trouble staying awake',\n",
        "}\n",
        "PRO_BINARY_TARGETS = list(PRO_BINARY_TARGETS_TO_NAME.values())\n",
        "\n",
        "# Mapping from model suffix to model name.\n",
        "PRO_SUFFIX_TO_MODEL = {\n",
        "    '_llm': 'PH-LLM w/ Adapter',\n",
        "    '_llm_few_shot': 'PH-LLM Few-shot',\n",
        "    '_llm_zero_shot': 'PH-LLM Zero-shot',\n",
        "    '_logreg_2914_data': 'LogReg',\n",
        "    '_cnn': 'CNN',\n",
        "}\n",
        "\n",
        "# Mapping from model name to color.\n",
        "PRO_MODEL_TO_COLOR = {\n",
        "    'PH-LLM w/ Adapter': COLOR_BLUE_DARK,\n",
        "    'PH-LLM Few-shot': COLOR_BLUE_MID,\n",
        "    'PH-LLM Zero-shot': COLOR_BLUE_LIGHT,\n",
        "    'Prevalence': COLOR_GRAY,\n",
        "    'LogReg': COLOR_ORANGE,\n",
        "    'CNN': COLOR_GREEN,\n",
        "}\n",
        "\n",
        "# Mapping from metric to their configs for plotting.\n",
        "# xlabel: The xlabel of subplot.\n",
        "# non_significant_indices: Target indices w/o significantly different\n",
        "#   performance.\n",
        "# significant_loc: The location of its significant mark denoted by (y, h) where\n",
        "#   y is the coordinate at which to start the annotation bracket, and h is the\n",
        "#   height of the annotation bracket.\n",
        "# legend_loc: The location of legend.\n",
        "PRO_METRIC_TO_CONFIGS = {\n",
        "    'auc': {\n",
        "        'xlabel': 'AUROC',\n",
        "        'non_significant_indices': [12, 13],\n",
        "        'significant_loc': [0.8, 0.02],\n",
        "        'legend_loc': 'lower left',\n",
        "    },\n",
        "    'auprc': {\n",
        "        'xlabel': 'AUPRC',\n",
        "        'non_significant_indices': [12, 13, 15],\n",
        "        'significant_loc': [0.3, 0.01],\n",
        "        'legend_loc': 'lower center',\n",
        "    },\n",
        "}\n",
        "\n",
        "# Column names in dataframes.\n",
        "PRO_COL_MODEL = 'Model'\n",
        "PRO_COL_TARGET = 'Target'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "mgPSz_4mnX33"
      },
      "cell_type": "markdown",
      "source": [
        "### Data loading utilities"
      ]
    },
    {
      "metadata": {
        "id": "Q4yT-2KMnaXL"
      },
      "cell_type": "code",
      "source": [
        "def tsv_to_df(filepath: str) -\u003e pd.DataFrame:\n",
        "  \"\"\"Returns a dataframe from a TSV file.\"\"\"\n",
        "  with open(filepath, mode='r') as f:\n",
        "    df = pd.read_csv(f, sep='\\t')\n",
        "  return df\n",
        "\n",
        "\n",
        "def load_ratings_df(\n",
        "    vertical: Vertical,\n",
        "    rating_source: RatingsSource,\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Returns a ratings dataframe for the given vertical and rating source.\"\"\"\n",
        "  base_dir = _VERTICAL_TO_BASE_DIR[vertical]\n",
        "  filename = f'{vertical.value.lower()}_{rating_source.value}_ratings.tsv'\n",
        "  filepath = os.path.join(base_dir, filename)\n",
        "  return tsv_to_df(filepath)\n",
        "\n",
        "\n",
        "def extract_pro_target_and_model(\n",
        "    raw_name: str,\n",
        "    prefixes: list[str] | None = None,\n",
        "    suffixes: list[str] | None = None,\n",
        ") -\u003e tuple[str, str]:\n",
        "  if prefixes is None:\n",
        "    prefixes = list(PRO_BINARY_TARGETS_TO_NAME.keys())\n",
        "  if suffixes is None:\n",
        "    suffixes = list(PRO_SUFFIX_TO_MODEL.keys())\n",
        "  for p in prefixes:\n",
        "    if raw_name.startswith(p):\n",
        "      s = raw_name[len(p) :]\n",
        "      if s in suffixes:\n",
        "        return PRO_BINARY_TARGETS_TO_NAME[p], PRO_SUFFIX_TO_MODEL[s]\n",
        "      return p, ''\n",
        "  raise ValueError(f'{raw_name} does not start from target name.')\n",
        "\n",
        "\n",
        "def load_pro_df(\n",
        "    models_of_interest: list[str],\n",
        "    prediction_path: str = PRO_BOOTSTRAP_PATH,\n",
        "    prevalence_path: str = PRO_PREVALENCE_PATH,\n",
        ") -\u003e pd.DataFrame:\n",
        "  # Load bootstrapping results and prevalence info.\n",
        "  df_pro = tsv_to_df(prediction_path)\n",
        "  df_pro_prevalence = tsv_to_df(prevalence_path)\n",
        "\n",
        "  # Add readable model and target name.\n",
        "  df_pro[[PRO_COL_TARGET, PRO_COL_MODEL]] = df_pro['prediction_name'].apply(\n",
        "      lambda x: pd.Series(extract_pro_target_and_model(x))\n",
        "  )\n",
        "  # Filter in results from models of interest.\n",
        "  df_pro = df_pro[df_pro[PRO_COL_MODEL].isin(models_of_interest)]\n",
        "  # Order rows by model then target (to align with barplot).\n",
        "  target_category_type = pd.CategoricalDtype(\n",
        "      categories=PRO_BINARY_TARGETS, ordered=True\n",
        "  )\n",
        "  model_category_type = pd.CategoricalDtype(categories=models_of_interest, ordered=True)\n",
        "  df_pro[PRO_COL_TARGET] = df_pro[PRO_COL_TARGET].astype(target_category_type)\n",
        "  df_pro[PRO_COL_MODEL] = df_pro[PRO_COL_MODEL].astype(model_category_type)\n",
        "  df_pro = df_pro.sort_values(by=[PRO_COL_MODEL, PRO_COL_TARGET]).reset_index(\n",
        "      drop=True\n",
        "  )\n",
        "  return df_pro, df_pro_prevalence"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "x3WQcCXTnzZi"
      },
      "cell_type": "markdown",
      "source": [
        "### Significance test utilities"
      ]
    },
    {
      "metadata": {
        "id": "T_nI-3AOIevB"
      },
      "cell_type": "code",
      "source": [
        "def _is_pair_significant(\n",
        "    grouped_df: pd.DataFrame,\n",
        "    source_col: str,\n",
        "    source_a: str,\n",
        "    source_b: str,\n",
        "    rating_column: str,\n",
        ") -\u003e pd.Series:\n",
        "  result = grouped_df.apply(\n",
        "      lambda x: (\n",
        "          sum(x[source_col] == source_a) + sum(x[source_col] == source_b),\n",
        "          scipy.stats.ranksums(\n",
        "              x[x[source_col] == source_a][rating_column],\n",
        "              x[x[source_col] == source_b][rating_column],\n",
        "          ),\n",
        "      ),\n",
        "      include_groups=False,\n",
        "  )\n",
        "  return result\n",
        "\n",
        "\n",
        "def _split_pair_test(pair_test: pd.Series) -\u003e tuple[\n",
        "    pd.Series,\n",
        "    pd.Series,\n",
        "    pd.Series,\n",
        "]:\n",
        "  result = (\n",
        "      pair_test.apply(lambda x: x[0]),\n",
        "      pair_test.apply(lambda x: x[1].statistic),\n",
        "      pair_test.apply(lambda x: x[1].pvalue),\n",
        "  )\n",
        "  return result\n",
        "\n",
        "\n",
        "def _unroll_pair_tests(\n",
        "    pair_tests: dict[tuple[str, str], pd.Series],\n",
        ") -\u003e pd.DataFrame:\n",
        "  unrolled_tests = []\n",
        "  unrolled_columns = []\n",
        "  for (source_a, source_b), pair_test in pair_tests.items():\n",
        "    n, statistic, pvalue = _split_pair_test(pair_test)\n",
        "    corrected_pvalue = pd.Series(\n",
        "        statsmodels.stats.multitest.fdrcorrection(pvalue.values)[1],\n",
        "        index=pvalue.index,\n",
        "    )\n",
        "    is_sig = corrected_pvalue \u003c 0.05\n",
        "    effect_size = statistic / np.sqrt(n)\n",
        "    unrolled_tests.extend([n, statistic, corrected_pvalue, is_sig, effect_size])\n",
        "    col_prefix = f'{source_a} vs {source_b}'\n",
        "    unrolled_columns.extend([\n",
        "        f'{col_prefix} n',\n",
        "        f'{col_prefix} statistic',\n",
        "        f'{col_prefix} p-value',\n",
        "        f'{col_prefix} significance',\n",
        "        f'{col_prefix} effect size',\n",
        "    ])\n",
        "  unrolled_test_df = pd.concat(unrolled_tests, axis=1)\n",
        "  unrolled_test_df.columns = unrolled_columns\n",
        "  return unrolled_test_df\n",
        "\n",
        "\n",
        "def significance_test(\n",
        "    df,\n",
        "    groupby_column,\n",
        "    rating_column: str = 'rating',\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Performs significance testing for each group using Wilcoxon rank-sum test.\n",
        "\n",
        "  The signigicance testing is done for all pairs. The p-values are adjusted with\n",
        "  FDR correction for multiple hypothesis testing.\n",
        "\n",
        "  Args:\n",
        "    df: The dataframe to perform significance testing on.\n",
        "    groupby_column: The column to group by.\n",
        "    rating_column: The column to use for ratings.\n",
        "\n",
        "  Returns:\n",
        "    A dataframe with statistic, p-value, and significance result for each group\n",
        "      and test.\n",
        "  \"\"\"\n",
        "  # Group the dataframe by the specified column.\n",
        "  grouped_df = df.groupby(groupby_column)\n",
        "\n",
        "  # Perform significance testing for each rater pair.\n",
        "  is_significant_pairs: dict[tuple[str, str], pd.Series] = {}\n",
        "  sources = sorted(df[COL_CONVERSATION_SOURCE].unique())\n",
        "  for i, source_a in enumerate(sources[:-1]):\n",
        "    for source_b in sources[i + 1 :]:\n",
        "      is_significant_pairs[(source_a, source_b)] = _is_pair_significant(\n",
        "          grouped_df=grouped_df,\n",
        "          source_col=COL_CONVERSATION_SOURCE,\n",
        "          source_a=source_a,\n",
        "          source_b=source_b,\n",
        "          rating_column=rating_column,\n",
        "      )\n",
        "      is_significant_pairs[(source_b, source_a)] = _is_pair_significant(\n",
        "          grouped_df=grouped_df,\n",
        "          source_col=COL_CONVERSATION_SOURCE,\n",
        "          source_a=source_b,\n",
        "          source_b=source_a,\n",
        "          rating_column=rating_column,\n",
        "      )\n",
        "\n",
        "  # Concatenate the results together into a dataframe.\n",
        "  stat_results = _unroll_pair_tests(is_significant_pairs)\n",
        "  return stat_results\n",
        "\n",
        "\n",
        "def get_sig_sections(sig_df: pd.DataFrame) -\u003e dict[str, list[tuple[str, str]]]:\n",
        "  \"\"\"Returns a dictionary mapping sections to their statsig diff pairs.\"\"\"\n",
        "  sig_pairs = collections.defaultdict(list)\n",
        "  check_pairs = [\n",
        "      (\n",
        "          (KEY_GEMINI_ULTRA, KEY_PHLLM),\n",
        "          f'{KEY_GEMINI_ULTRA} vs {KEY_PHLLM} significance',\n",
        "      ),\n",
        "      (\n",
        "          (KEY_HUMAN_EXPERT, KEY_PHLLM),\n",
        "          f'{KEY_HUMAN_EXPERT} vs {KEY_PHLLM} significance',\n",
        "      ),\n",
        "  ]\n",
        "  for section in sig_df.index:\n",
        "    for pair, label in check_pairs:\n",
        "      if label not in sig_df.columns:\n",
        "        continue\n",
        "      is_sig = sig_df[(sig_df.index == section)][label].values[0]\n",
        "      if is_sig:\n",
        "        sig_pairs[section].append(pair)\n",
        "  return sig_pairs\n",
        "\n",
        "\n",
        "def get_non_sig_sections(\n",
        "    sig_df: pd.DataFrame,\n",
        ") -\u003e dict[str, list[tuple[str, str]]]:\n",
        "  \"\"\"Returns a dictionary mapping sections to their non statsig diff pairs.\"\"\"\n",
        "  sig_pairs = collections.defaultdict(list)\n",
        "  check_pairs = []\n",
        "  all_sources = list(CONVERSATION_SOURCE_KEY_TO_LABEL)\n",
        "  for source_a in all_sources:\n",
        "    for source_b in all_sources:\n",
        "      if source_a == source_b:\n",
        "        continue\n",
        "      else:\n",
        "        check_pairs.append(\n",
        "            ((source_a, source_b), f'{source_a} vs {source_b} significance')\n",
        "        )\n",
        "\n",
        "  for section in sig_df.index:\n",
        "    sig_pairs[section] = []\n",
        "    for pair, label in check_pairs:\n",
        "      if label not in sig_df.columns:\n",
        "        continue\n",
        "      is_sig = sig_df[(sig_df.index == section)][label].values[0]\n",
        "      if not is_sig:\n",
        "        sig_pairs[section].append(pair)\n",
        "  return sig_pairs"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "UtDxnwpSDFCg"
      },
      "cell_type": "markdown",
      "source": [
        "### Bootstrapping utilities"
      ]
    },
    {
      "metadata": {
        "id": "Eldf6WT1DHA5"
      },
      "cell_type": "code",
      "source": [
        "# A function that computes a numeric outcome from label and prediction arrays.\n",
        "BootstrappableFn = Callable[[np.ndarray, np.ndarray], float]\n",
        "\n",
        "# Constants denoting the expected case and control values for binary encodings.\n",
        "BINARY_LABEL_CONTROL = 0\n",
        "BINARY_LABEL_CASE = 1\n",
        "\n",
        "# The maximum number of threads/workers.\n",
        "_MAX_PARALLEL_WORKERS = 10\n",
        "\n",
        "# Represents a numpy array of indices for a single bootstrap sample.\n",
        "IndexSample = np.ndarray\n",
        "\n",
        "\n",
        "class Metric(abc.ABC):\n",
        "  \"\"\"Represents a callable wrapper class for a named metric function.\n",
        "\n",
        "  Attributes:\n",
        "    name: The metric's name.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name: str, fn: BootstrappableFn) -\u003e None:\n",
        "    \"\"\"Initializes the metric.\n",
        "\n",
        "    Args:\n",
        "      name: The metric's name.\n",
        "      fn: A function that computes an outcome from label and prediction arrays.\n",
        "        The function's signature should accept a `y_true` label array and a\n",
        "        `y_pred` model prediction array. This function is invoked when the\n",
        "        `Metric` instance is called.\n",
        "    \"\"\"\n",
        "    self._name: str = name\n",
        "    self._fn: BootstrappableFn = fn\n",
        "\n",
        "  @property\n",
        "  def name(self) -\u003e str:\n",
        "    \"\"\"The `Metric`'s name.\"\"\"\n",
        "    return self._name\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _validate(self, y_true: np.ndarray, y_pred: np.ndarray) -\u003e None:\n",
        "    \"\"\"Validates the `y_true` labels and `y_pred` predictions.\n",
        "\n",
        "    Note: Each prediction subarray `y_pred[i, ...]` at index `i` should\n",
        "    correspond to the `y_true[i]` label.\n",
        "\n",
        "    Args:\n",
        "      y_true: The ground truth label targets.\n",
        "      y_pred: The target predictions.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the first dimension of `y_true` and `y_pred` do not match.\n",
        "    \"\"\"\n",
        "    if y_true.shape[0] != y_pred.shape[0]:\n",
        "      raise ValueError(\n",
        "          '`y_true` and `y_pred` first dimension mismatch: '\n",
        "          f'{y_true.shape[0]} != {y_pred.shape[0]}'\n",
        "      )\n",
        "\n",
        "  def __call__(self, y_true: np.ndarray, y_pred: np.ndarray) -\u003e float:\n",
        "    \"\"\"Invokes the `Metric`'s function.\n",
        "\n",
        "    Args:\n",
        "      y_true: The ground truth label values.\n",
        "      y_pred: The target predictions.\n",
        "\n",
        "    Returns:\n",
        "      The result of the `Metric.fn(y_true, y_pred)`.\n",
        "    \"\"\"\n",
        "    self._validate(y_true, y_pred)\n",
        "    return self._fn(y_true, y_pred)\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return self.name\n",
        "\n",
        "\n",
        "class ContinuousMetric(Metric):\n",
        "  \"\"\"Represents a callable wrapper class for a named continuous label function.\n",
        "\n",
        "  Attributes:\n",
        "    name: The metric's name.\n",
        "  \"\"\"\n",
        "\n",
        "  # Note: This is a useful delegation since _validate is an @abc.abstractmethod.\n",
        "  def _validate(  # pylint: disable=useless-super-delegation\n",
        "      self,\n",
        "      y_true: np.ndarray,\n",
        "      y_pred: np.ndarray,\n",
        "  ) -\u003e None:\n",
        "    \"\"\"Validates the `y_true` labels and `y_pred` predictions.\n",
        "\n",
        "    Args:\n",
        "      y_true: The ground truth label values.\n",
        "      y_pred: The target predictions.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the first dimension of `y_true` and `y_pred` do not match.\n",
        "    \"\"\"\n",
        "    super()._validate(y_true, y_pred)\n",
        "\n",
        "\n",
        "class BinaryMetric(Metric):\n",
        "  \"\"\"Represents a callable wrapper class for a named binary label function.\n",
        "\n",
        "  This class asserts that the provided `y_true` labels are binary targets in\n",
        "  `{0, 1}` and that `y_true` contains at least one element in each class, i.e.,\n",
        "  not all samples are from the same class.\n",
        "\n",
        "  Attributes:\n",
        "    name: The metric's name.\n",
        "  \"\"\"\n",
        "\n",
        "  def _validate(self, y_true: np.ndarray, y_pred: np.ndarray) -\u003e None:\n",
        "    \"\"\"Validates the `y_true` labels and `y_pred` predictions.\n",
        "\n",
        "    Args:\n",
        "      y_true: The ground truth label values.\n",
        "      y_pred: The target predictions.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the first dimension of `y_true` and `y_pred` do not match.\n",
        "      ValueError: If `y_true` labels are nonbinary, i.e., not all values are in\n",
        "        `{BINARY_LABEL_CONTROL, BINARY_LABEL_CASE}` or if `y_true` does not\n",
        "        contain at least one element from each class.\n",
        "    \"\"\"\n",
        "    super()._validate(y_true, y_pred)\n",
        "    if not is_valid_binary_label(y_true):\n",
        "      raise ValueError(\n",
        "          '`y_true` labels must be in `{BINARY_LABEL_CONTROL, '\n",
        "          'BINARY_LABEL_CASE}` and have at least one element from '\n",
        "          f'each class; found: {y_true}'\n",
        "      )\n",
        "\n",
        "\n",
        "def is_binary(metric: Metric) -\u003e bool:\n",
        "  \"\"\"Whether `metric` is a metric computed with binary `y_true` labels.\"\"\"\n",
        "  return isinstance(metric, BinaryMetric)\n",
        "\n",
        "\n",
        "def is_valid_binary_label(array: np.ndarray) -\u003e bool:\n",
        "  \"\"\"Whether `array` is a \"valid\" binary label array for bootstrapping.\n",
        "\n",
        "  We define a valid binary label array as an array that contains only binary\n",
        "  values, i.e., `{BINARY_LABEL_CONTROL, BINARY_LABEL_CASE}`, and contains at\n",
        "  least one value from each class.\n",
        "\n",
        "  Args:\n",
        "    array: A numpy array.\n",
        "\n",
        "  Returns:\n",
        "    Whether `array` is a \"valid\" binary label array.\n",
        "  \"\"\"\n",
        "  is_case_mask = array == BINARY_LABEL_CASE\n",
        "  is_control_mask = array == BINARY_LABEL_CONTROL\n",
        "  return (\n",
        "      np.any(is_case_mask)\n",
        "      and np.any(is_control_mask)\n",
        "      and np.all(np.logical_or(is_case_mask, is_control_mask))\n",
        "  )\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class NamedArray:\n",
        "  \"\"\"Represents a named numpy array.\n",
        "\n",
        "  Attributes:\n",
        "    name: The array name.\n",
        "    values: A numpy array.\n",
        "  \"\"\"\n",
        "\n",
        "  name: str\n",
        "  values: np.ndarray\n",
        "\n",
        "  def __post_init__(self):\n",
        "    if not self.name:\n",
        "      raise ValueError('`name` must be specified.')\n",
        "\n",
        "  def __len__(self) -\u003e int:\n",
        "    return len(self.values)\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return f'{self.__class__.__name__}({self.name})'\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class Label(NamedArray):\n",
        "  \"\"\"Represents a named numpy array of ground truth label targets.\n",
        "\n",
        "  Attributes:\n",
        "    name: The label name.\n",
        "    values: A numpy array containing ground truth label targets.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class Prediction(NamedArray):\n",
        "  \"\"\"Represents a named numpy array of target predictions.\n",
        "\n",
        "  Attributes:\n",
        "    model_name: The name of the model that generated the predictions.\n",
        "    name: The name of the predictions (e.g., the prediction column).\n",
        "    values: A numpy array containing model predictions.\n",
        "  \"\"\"\n",
        "\n",
        "  model_name: str\n",
        "\n",
        "  def __post_init__(self):\n",
        "    super().__post_init__()\n",
        "    if not self.model_name:\n",
        "      raise ValueError('`model_name` must be specified.')\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return f'{self.__class__.__name__}({self.model_name}.{self.name})'\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class SampleMean:\n",
        "  \"\"\"Represents an estimate of the population mean for a given sample.\n",
        "\n",
        "  Attributes:\n",
        "    mean: The mean of a given sample.\n",
        "    stddev: The standard deviation of the sample mean.\n",
        "    num_samples: The number of samples used to calculate `mean` and `stddev`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `num_samples` is not \u003e= `1`.\n",
        "    ValueError: If `stddev` is not `0` when `num_samples` is `1`.\n",
        "  \"\"\"\n",
        "\n",
        "  mean: float\n",
        "  stddev: float\n",
        "  num_samples: int\n",
        "\n",
        "  def __post_init__(self):\n",
        "    # Ensure we have a valid number of samples.\n",
        "    if self.num_samples \u003c 1:\n",
        "      raise ValueError(f'`num_samples` must be \u003e= `1`: {self.num_samples}')\n",
        "\n",
        "    # Ensure the standard deviation is 0 given a single sample.\n",
        "    if self.num_samples == 1 and self.stddev != 0.0:\n",
        "      raise ValueError(\n",
        "          f'`stddev` must be `0` if `num_samples` is `1`: {self.stddev:0.4f}'\n",
        "      )\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return f'{self.mean:0.4f} (SD={self.stddev:0.4f}, n={self.num_samples})'\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class ConfidenceInterval(SampleMean):\n",
        "  \"\"\"Represents a confidence interval (CI) for a sample mean.\n",
        "\n",
        "  Attributes:\n",
        "    mean: The mean of a given sample.\n",
        "    stddev: The standard deviation of the sample mean.\n",
        "    num_samples: The number of samples used to calculate `mean` and `stddev`.\n",
        "    level: The confidence level at which the CI is calculated (e.g., 95).\n",
        "    ci_lower: The lower limit of the `level` confidence interval.\n",
        "    ci_upper: The upper limit of the `level` confidence interval.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `num_samples` is not \u003e= `1`.\n",
        "    ValueError: If `stddev` is not `0` when `num_samples` is `1`.\n",
        "    ValueError: If `level` is not in range (0, 100].\n",
        "    ValueError: If `ci_lower` or `ci_upper` does not match not `mean` when\n",
        "      `num_samples` is `1`.\n",
        "  \"\"\"\n",
        "\n",
        "  level: float\n",
        "  ci_lower: float\n",
        "  ci_upper: float\n",
        "\n",
        "  def __post_init__(self):\n",
        "    super().__post_init__()\n",
        "    # Ensure we have a valid confidence level.\n",
        "    if not 0 \u003c self.level \u003c= 100:\n",
        "      raise ValueError(f'`level` must be in range (0, 100]: {self.level:0.2f}')\n",
        "\n",
        "    # Ensure confidence intervals match the sample mean given a single sample.\n",
        "    if self.num_samples == 1:\n",
        "      if (self.ci_lower != self.mean) or (self.ci_upper != self.mean):\n",
        "        raise ValueError(\n",
        "            '`ci_lower` and `ci_upper` must match `mean` if `num_samples` is '\n",
        "            f'1: mean={self.mean:0.4f}, ci_lower={self.ci_lower:0.4f}, '\n",
        "            f'ci_upper={self.ci_upper:0.4f}'\n",
        "        )\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return (\n",
        "        f'{self.mean:0.4f} (SD={self.stddev:0.4f}, n={self.num_samples}, '\n",
        "        f'{self.level:0\u003e6.2f}% CI=[{self.ci_lower:0.4f}, '\n",
        "        f'{self.ci_upper:0.4f}])'\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class Result:\n",
        "  \"\"\"Represents a bootstrapped metric result for an individual model.\n",
        "\n",
        "  Attributes:\n",
        "    model_name: The model's name.\n",
        "    prediction_name: The model's prediction name (e.g., the model head's name or\n",
        "      the label name used in training).\n",
        "    metric_name: The metric's name.\n",
        "    ci: A confidence interval describing the distribution of metric samples.\n",
        "  \"\"\"\n",
        "\n",
        "  model_name: str\n",
        "  prediction_name: str\n",
        "  metric_name: str\n",
        "  ci: ConfidenceInterval\n",
        "\n",
        "  def __post_init__(self):\n",
        "    # Ensure model, prediction, and metric names are specified.\n",
        "    if not self.model_name:\n",
        "      raise ValueError('`model_name` must be specified.')\n",
        "    if not self.prediction_name:\n",
        "      raise ValueError('`prediction_name` must be specified.')\n",
        "    if not self.metric_name:\n",
        "      raise ValueError('`metric_name` must be specified.')\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return (\n",
        "        f'{self.model_name}.{self.prediction_name}: '\n",
        "        f'{self.metric_name}: {self.ci}'\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, order=False, frozen=True)\n",
        "class PairedResult:\n",
        "  \"\"\"Represents a paired bootstrapped metric result for two models.\n",
        "\n",
        "  Attributes:\n",
        "    model_name_a: The first model's name.\n",
        "    prediction_name_a: The first model's prediction name (e.g., the model head's\n",
        "      name or the label name used in training).\n",
        "    model_name_b: The second model's name.\n",
        "    prediction_name_b: The second model's prediction name (e.g., the model\n",
        "      head's name or the label name used in training).\n",
        "    metric_name: The metric's name.\n",
        "    ci: A confidence interval describing the distribution of differences between\n",
        "      the first and second models' metric samples.\n",
        "  \"\"\"\n",
        "\n",
        "  model_name_a: str\n",
        "  prediction_name_a: str\n",
        "  model_name_b: str\n",
        "  prediction_name_b: str\n",
        "  metric_name: str\n",
        "  ci: ConfidenceInterval\n",
        "\n",
        "  def __post_init__(self):\n",
        "    # Ensure model, prediction, and metric names are specified.\n",
        "    if not self.model_name_a:\n",
        "      raise ValueError('`model_name_a` must be specified.')\n",
        "    if not self.prediction_name_a:\n",
        "      raise ValueError('`prediction_name_a` must be specified.')\n",
        "    if not self.model_name_b:\n",
        "      raise ValueError('`model_name_b` must be specified.')\n",
        "    if not self.prediction_name_b:\n",
        "      raise ValueError('`prediction_name_b` must be specified.')\n",
        "    if not self.metric_name:\n",
        "      raise ValueError('`metric_name` must be specified.')\n",
        "\n",
        "  def __str__(self) -\u003e str:\n",
        "    return (\n",
        "        f'({self.model_name_a}.{self.prediction_name_a} - '\n",
        "        f'{self.model_name_b}.{self.prediction_name_b}): '\n",
        "        f'{self.metric_name}: {self.ci}'\n",
        "    )\n",
        "\n",
        "\n",
        "def _compute_confidence_interval(\n",
        "    samples: np.ndarray,\n",
        "    ci_level: float,\n",
        ") -\u003e ConfidenceInterval:\n",
        "  \"\"\"Computes the mean, standard deviation, and confidence interval for samples.\n",
        "\n",
        "  Args:\n",
        "    samples: A boostrapped array of observed sample values.\n",
        "    ci_level: The confidence level/width of the desired confidence interval.\n",
        "\n",
        "  Returns:\n",
        "    A `Result` containing the mean, standard deviation, and the `ci_level`%\n",
        "    confidence interval for the observed sample values.\n",
        "  \"\"\"\n",
        "  sample_mean = np.mean(samples, axis=0)\n",
        "  sample_std = np.std(samples, axis=0)\n",
        "\n",
        "  lower_percentile = (100 - ci_level) / 2\n",
        "  upper_percentile = 100 - lower_percentile\n",
        "  percentiles = [lower_percentile, upper_percentile]\n",
        "  ci_lower, ci_upper = np.percentile(a=samples, q=percentiles, axis=0)\n",
        "\n",
        "  ci = ConfidenceInterval(\n",
        "      mean=sample_mean,\n",
        "      stddev=sample_std,\n",
        "      num_samples=len(samples),\n",
        "      level=ci_level,\n",
        "      ci_lower=ci_lower,\n",
        "      ci_upper=ci_upper,\n",
        "  )\n",
        "\n",
        "  return ci\n",
        "\n",
        "\n",
        "def _generate_sample_indices(\n",
        "    label: Label,\n",
        "    is_binary: bool,\n",
        "    num_bootstrap: int,\n",
        "    seed: int,\n",
        ") -\u003e List[IndexSample]:\n",
        "  \"\"\"Returns a list of `num_bootstrap` randomly sampled bootstrap indices.\n",
        "\n",
        "  Args:\n",
        "    label: The ground truth label targets.\n",
        "    is_binary: Whether to generate valid binary samples; i.e., each index sample\n",
        "      contains at least one index corresponding to a label from each class.\n",
        "    num_bootstrap: The number of bootstrap indices to generate.\n",
        "    seed: The random seed; set prior to generating bootstrap indices.\n",
        "\n",
        "  Returns:\n",
        "    A list of `num_bootstrap` bootstrap sample indices.\n",
        "  \"\"\"\n",
        "  rng = np.random.default_rng(seed)\n",
        "  num_observations = len(label)\n",
        "  sample_indices = []\n",
        "  while len(sample_indices) \u003c num_bootstrap:\n",
        "    index = rng.integers(0, high=num_observations, size=num_observations)\n",
        "    sample_true = label.values[index]\n",
        "    # If computing a binary metric, skip indices that result in invalid labels.\n",
        "    if is_binary and not is_valid_binary_label(sample_true):\n",
        "      continue\n",
        "    sample_indices.append(index)\n",
        "  return sample_indices\n",
        "\n",
        "\n",
        "def _compute_metric_samples(\n",
        "    metric: Metric,\n",
        "    label: Label,\n",
        "    predictions: Sequence[Prediction],\n",
        "    sample_indices: Sequence[np.ndarray],\n",
        ") -\u003e Dict[str, np.ndarray]:\n",
        "  \"\"\"Generates `num_bootstrap` metric samples for each `Prediction`.\n",
        "\n",
        "  Note: This method assumes that label and prediction values are orded so that\n",
        "  the value at index `i` in a given `Prediction` corresponds to the label value\n",
        "  at index `i` in `label`. Both the `Label` and `Prediction` arrays are indexed\n",
        "  using the given `sample_indices`.\n",
        "\n",
        "  Args:\n",
        "    metric: An instance of a bootstrappable `Metric`; used to compute samples.\n",
        "    label: The ground truth label targets.\n",
        "    predictions: A list of target predictions from a set of models.\n",
        "    sample_indices: An array of bootstrap sample indices. If empty, returns the\n",
        "      single value computed on the entire dataset for each prediction.\n",
        "\n",
        "  Returns:\n",
        "    A mapping of model names to the corresponding metric samples array.\n",
        "  \"\"\"\n",
        "  if not sample_indices:\n",
        "    metric_samples = {}\n",
        "    for prediction in predictions:\n",
        "      value = metric(label.values, prediction.values)\n",
        "      metric_samples[prediction.model_name] = np.asarray([value])\n",
        "    return metric_samples\n",
        "\n",
        "  metric_samples = {prediction.model_name: [] for prediction in predictions}\n",
        "  for index in sample_indices:\n",
        "    sample_true = label.values[index]\n",
        "    for prediction in predictions:\n",
        "      sample_value = metric(sample_true, prediction.values[index])\n",
        "      metric_samples[prediction.model_name].append(sample_value)\n",
        "\n",
        "  metric_samples = {\n",
        "      name: np.asarray(samples) for name, samples in metric_samples.items()\n",
        "  }\n",
        "\n",
        "  return metric_samples\n",
        "\n",
        "\n",
        "def _compute_all_metric_samples(\n",
        "    metrics: Sequence[Metric],\n",
        "    contains_binary_metric: bool,\n",
        "    label: Label,\n",
        "    predictions: Sequence[Prediction],\n",
        "    num_bootstrap: int,\n",
        "    seed: int,\n",
        ") -\u003e Dict[str, Dict[str, np.ndarray]]:\n",
        "  \"\"\"Generates `num_bootstrap` samples for each `Prediction` and `Metric`.\n",
        "\n",
        "  Args:\n",
        "    metrics: A sequence of a bootstrappable `Metric` instances.\n",
        "    contains_binary_metric: Whether the set of metrics contains a binary metric.\n",
        "    label: The ground truth label targets.\n",
        "    predictions: A list of target predictions from a set of models.\n",
        "    num_bootstrap: The number of bootstrap iterations.\n",
        "    seed: The random seed; set prior to generating bootstrap indices.\n",
        "\n",
        "  Returns:\n",
        "    A mapping of metric names to model-sample dictionaries.\n",
        "  \"\"\"\n",
        "  sample_indices = _generate_sample_indices(\n",
        "      label,\n",
        "      contains_binary_metric,\n",
        "      num_bootstrap,\n",
        "      seed,\n",
        "  )\n",
        "  metric_samples_kwargs = []\n",
        "  for metric in metrics:\n",
        "    metric_samples_kwargs.append({\n",
        "        'metric': metric,\n",
        "        'label': label,\n",
        "        'predictions': predictions,\n",
        "        'sample_indices': sample_indices,\n",
        "    })\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor(\n",
        "      max_workers=min(_MAX_PARALLEL_WORKERS, len(metrics))\n",
        "  ) as executor:\n",
        "    futures = [\n",
        "        executor.submit(_compute_metric_samples, **kwargs)\n",
        "        for kwargs in metric_samples_kwargs\n",
        "    ]\n",
        "    metric_samples = [future.result() for future in futures]\n",
        "\n",
        "  return {\n",
        "      metric.name: metric_sample\n",
        "      for metric, metric_sample in zip(metrics, metric_samples)\n",
        "  }\n",
        "\n",
        "\n",
        "def _process_metric_samples(\n",
        "    metric: Metric,\n",
        "    predictions: Sequence[Prediction],\n",
        "    model_names_to_metric_samples: Dict[str, np.ndarray],\n",
        "    ci_level: float,\n",
        ") -\u003e List[Result]:\n",
        "  \"\"\"Compute `ConfidenceInterval`s for metric samples across predictions.\"\"\"\n",
        "  results = []\n",
        "  for prediction in predictions:\n",
        "    metric_samples = model_names_to_metric_samples[prediction.model_name]\n",
        "    ci = _compute_confidence_interval(metric_samples, ci_level)\n",
        "    result = Result(prediction.model_name, prediction.name, metric.name, ci)\n",
        "    results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def _bootstrap(\n",
        "    metrics: Sequence[Metric],\n",
        "    contains_binary_metric: bool,\n",
        "    label: Label,\n",
        "    predictions: Sequence[Prediction],\n",
        "    num_bootstrap: int,\n",
        "    ci_level: float,\n",
        "    seed: int,\n",
        ") -\u003e Dict[str, List[Result]]:\n",
        "  \"\"\"Performs bootstrapping for all models using the given metrics.\n",
        "\n",
        "  Args:\n",
        "    metrics: A sequence of a bootstrappable `Metric` instances.\n",
        "    contains_binary_metric: Whether the set of metrics contains a binary metric.\n",
        "    label: The ground truth label targets.\n",
        "    predictions: A list of target predictions from a set of models.\n",
        "    num_bootstrap: The number of bootstrap iterations.\n",
        "    ci_level: The confidence level/width of the desired confidence interval.\n",
        "    seed: The random seed; set prior to generating bootstrap indices.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary mapping metric names to a list of `Result`s containing the mean\n",
        "    metric values of each model over `num_bootstrap` bootstrapping iterations.\n",
        "  \"\"\"\n",
        "  metric_to_model_to_samples = _compute_all_metric_samples(\n",
        "      metrics,\n",
        "      contains_binary_metric,\n",
        "      label,\n",
        "      predictions,\n",
        "      num_bootstrap,\n",
        "      seed,\n",
        "  )\n",
        "  process_metric_samples_kwargs = []\n",
        "  for metric in metrics:\n",
        "    process_metric_samples_kwargs.append({\n",
        "        'metric': metric,\n",
        "        'predictions': predictions,\n",
        "        'model_names_to_metric_samples': metric_to_model_to_samples[\n",
        "            metric.name\n",
        "        ],\n",
        "        'ci_level': ci_level,\n",
        "    })\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor(\n",
        "      max_workers=min(_MAX_PARALLEL_WORKERS, len(metrics))\n",
        "  ) as executor:\n",
        "    futures = [\n",
        "        executor.submit(_process_metric_samples, **kwargs)\n",
        "        for kwargs in process_metric_samples_kwargs\n",
        "    ]\n",
        "    metric_samples = [future.result() for future in futures]\n",
        "\n",
        "  return {\n",
        "      metric.name: metric_sample\n",
        "      for metric, metric_sample in zip(metrics, metric_samples)\n",
        "  }\n",
        "\n",
        "\n",
        "def validate_and_mask(\n",
        "    label: Label,\n",
        "    predictions: Sequence[Prediction],\n",
        "    mask: Optional[np.ndarray] = None,\n",
        ") -\u003e tuple[Label, list[Prediction]]:\n",
        "  \"\"\"Validates bootstrap argument shape and applies the mask if needed.\"\"\"\n",
        "  for prediction in predictions:\n",
        "    if len(label) != len(prediction):\n",
        "      raise ValueError('Label and prediction dimensions do not match.')\n",
        "  if mask is not None and len(mask) != len(label):\n",
        "    raise ValueError('Label and prediction dimensions do not match mask.')\n",
        "  if mask is not None:\n",
        "    label = Label(label.name, label.values[mask])\n",
        "    predictions = [\n",
        "        Prediction(\n",
        "            name=label.name, values=p.values[mask], model_name=p.model_name\n",
        "        )\n",
        "        for p in predictions\n",
        "    ]\n",
        "  return label, predictions  # pytype: disable=bad-return-type\n",
        "\n",
        "\n",
        "class PerformanceMetricsParallel:\n",
        "  \"\"\"A named collection of invocable, bootstrapable `Metric`s.\n",
        "\n",
        "  Initializes a class that applies the given `Metric` functions to new ground\n",
        "  truth labels and predictions. `Metric`s can be evaluated with and without\n",
        "  bootstrapping.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if an item in `metrics` is not of type `Metric`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      name: str,\n",
        "      metrics: Optional[list[Metric]] = None,\n",
        "  ) -\u003e None:\n",
        "    if metrics is None:\n",
        "      raise ValueError('No metric is provided.')\n",
        "\n",
        "    for metric in metrics:\n",
        "      if not isinstance(metric, Metric):\n",
        "        raise ValueError('Invalid metric value: must be of class `Metric`.')\n",
        "\n",
        "    if len(metrics) != len({metric.name for metric in metrics}):\n",
        "      raise ValueError(f'Metric names must be unique: {metrics}')\n",
        "\n",
        "    self.name = name\n",
        "    self.metrics = metrics\n",
        "    self.contains_binary = any(is_binary(m) for m in metrics)\n",
        "\n",
        "  def compute(\n",
        "      self,\n",
        "      label: Label,\n",
        "      predictions: Sequence[Prediction],\n",
        "      mask: Optional[np.ndarray] = None,\n",
        "      n_bootstrap: int = 0,\n",
        "      conf_interval: float = 95,\n",
        "      seed: int = 42,\n",
        "  ) -\u003e dict[str, list[Result]]:\n",
        "    \"\"\"Evaluates all metrics using the given labels and predictions.\n",
        "\n",
        "    Args:\n",
        "      label: The ground truth label targets.\n",
        "      predictions: A list of target predictions from a set of models.\n",
        "      mask: A boolean mask; applied to `y_true` and `y_pred`.\n",
        "      n_bootstrap: An integer denoting the number of bootstrap iterations for\n",
        "        each evaluation metric.\n",
        "      conf_interval: A float denoting the width of confidence interval.\n",
        "      seed: An int denoting the seed for the PRNG.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of bootstrapped metrics keyed on metric name with\n",
        "      `Result` values.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the dimensions of `y_true`, `y_pred`, or `mask` do not\n",
        "      match, or labels are not in {0 , 1}.\n",
        "    \"\"\"\n",
        "    label, predictions = validate_and_mask(label, predictions, mask)\n",
        "    metric_results = _bootstrap(\n",
        "        self.metrics,\n",
        "        contains_binary_metric=self.contains_binary,\n",
        "        label=label,\n",
        "        predictions=predictions,\n",
        "        num_bootstrap=n_bootstrap,\n",
        "        ci_level=conf_interval,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    return metric_results\n",
        "\n",
        "\n",
        "def result_to_record(result: Result) -\u003e dict[str, Any]:\n",
        "  \"\"\"Converts a `Result` into a flattened dictionary record.\"\"\"\n",
        "  record = {\n",
        "      'model_name': result.model_name,\n",
        "      'prediction_name': result.prediction_name,\n",
        "      'metric_name': result.metric_name,\n",
        "      'mean': result.ci.mean,\n",
        "      'stddev': result.ci.stddev,\n",
        "      'num_samples': result.ci.num_samples,\n",
        "      'ci_level': result.ci.level,\n",
        "      'ci_lower': result.ci.ci_lower,\n",
        "      'ci_upper': result.ci.ci_upper,\n",
        "  }\n",
        "  return record\n",
        "\n",
        "\n",
        "def results_map_to_df(\n",
        "    metrics_to_results: Mapping[str, Sequence[Result]],\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Converts a metrics to `Result`s mapping to a dataframe.\"\"\"\n",
        "  all_results = []\n",
        "  for metric_results in metrics_to_results.values():\n",
        "    all_results.extend(metric_results)\n",
        "  records = [result_to_record(result) for result in all_results]\n",
        "  return pd.DataFrame.from_records(records)\n",
        "\n",
        "\n",
        "def build_bootstrap_inputs(\n",
        "    df: pd.DataFrame,\n",
        "    label_column: str,\n",
        "    prediction_columns: list[str],\n",
        "    id_col: str,\n",
        ") -\u003e tuple[Label, list[Prediction]]:\n",
        "  \"\"\"Returns a bootstrapping label and prediction list.\"\"\"\n",
        "  if (\n",
        "      (label_column in prediction_columns)\n",
        "      or (id_col in prediction_columns)\n",
        "      or (len(prediction_columns) != len(set(prediction_columns)))\n",
        "  ):\n",
        "    raise ValueError('Label and ID columns must be unique.')\n",
        "  expected_columns = {id_col, label_column, *prediction_columns}\n",
        "  column_diff = expected_columns - set(df.columns)\n",
        "  if column_diff:\n",
        "    raise ValueError(f'Missing expected dataframe columns: {column_diff}.')\n",
        "\n",
        "  label = Label(label_column, df[label_column].to_numpy())\n",
        "  preds = []\n",
        "  for pred_col in prediction_columns:\n",
        "    pred_values = df[pred_col].to_numpy()\n",
        "    preds.append(Prediction(label_column, pred_values, pred_col))\n",
        "  return label, preds\n",
        "\n",
        "\n",
        "def compute(\n",
        "    metrics: list[Metric],\n",
        "    label: Label,\n",
        "    preds: Sequence[Prediction],\n",
        "    n_bootstrap: int,\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Bootstraps predictions using `metrics` and returns a result dataframe.\"\"\"\n",
        "  perf = PerformanceMetricsParallel(name='', metrics=metrics)\n",
        "  metric_results = perf.compute(label, preds, n_bootstrap=n_bootstrap)\n",
        "  results_df = results_map_to_df(metric_results)\n",
        "  return results_df\n",
        "\n",
        "\n",
        "def bootstrap_rater_pairs_binary(\n",
        "    df: pd.DataFrame,\n",
        ") -\u003e pd.DataFrame:\n",
        "  df = df.copy()\n",
        "  df[DERIVED_COL_KEY_NO_RATER] = df.apply(_key_no_rater_column, axis=1)\n",
        "  rater_pairs = get_rater_pairs(df)\n",
        "  df_pivot = df.pivot_table(\n",
        "      index=DERIVED_COL_KEY_NO_RATER,\n",
        "      columns=COL_RATER,\n",
        "      values=COL_RATING,\n",
        "  ).reset_index()\n",
        "  bs_dfs = []\n",
        "  for rater_a, rater_b in rater_pairs:\n",
        "    sub_df = (\n",
        "        df_pivot[[DERIVED_COL_KEY_NO_RATER, rater_a, rater_b]]\n",
        "        .dropna()\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    label, preds = build_bootstrap_inputs(\n",
        "        df=sub_df,\n",
        "        label_column=rater_a,\n",
        "        prediction_columns=[rater_b],\n",
        "        id_col=DERIVED_COL_KEY_NO_RATER,\n",
        "    )\n",
        "    results_df = compute(\n",
        "        metrics=BS_METRICS_BINARY,\n",
        "        label=label,\n",
        "        preds=preds,\n",
        "        n_bootstrap=1000,\n",
        "    )\n",
        "    bs_dfs.append(results_df)\n",
        "  return pd.concat(bs_dfs).reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8nnCjFchPj6i"
      },
      "cell_type": "code",
      "source": [
        "def gwet_ac2(ratings_a: np.ndarray, ratings_b: np.ndarray) -\u003e float:\n",
        "  \"\"\"Calculates Gwet's AC2 coefficient for two sets of nominal ratings.\n",
        "\n",
        "  Args:\n",
        "      ratings_a: A numpy array of ratings from rater A.\n",
        "      ratings_b: A numpy array of ratings from rater B.\n",
        "\n",
        "  Returns:\n",
        "      The AC1 inter-rater reliability coefficient.\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame({'rater A': ratings_a, 'rater B': ratings_b})\n",
        "  cac = irrCAC.raw.CAC(df, weights='quadratic')\n",
        "  return cac.gwet()['est']['coefficient_value']\n",
        "\n",
        "\n",
        "def sample_count(ratings_a: np.ndarray, ratings_b: np.ndarray) -\u003e float:\n",
        "  del ratings_b  # Unused.\n",
        "  return len(ratings_a)\n",
        "\n",
        "\n",
        "BS_METRICS_BINARY: list[Metric] = [\n",
        "    ContinuousMetric(\n",
        "        'gwet_ac2',\n",
        "        gwet_ac2,\n",
        "    ),\n",
        "    ContinuousMetric(\n",
        "        'sample_count',\n",
        "        sample_count,\n",
        "    ),\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2On6-95Onftf"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting utilities"
      ]
    },
    {
      "metadata": {
        "id": "eTAu73uX5ZCM"
      },
      "cell_type": "code",
      "source": [
        "def _get_label_position(\n",
        "    labels: list[text.Text],\n",
        "    target_label: str,\n",
        ") -\u003e int:\n",
        "  \"\"\"Returns the position of the taget label in the given labels.\"\"\"\n",
        "  for i, label in enumerate(labels):\n",
        "    if label.get_text() == target_label:\n",
        "      return i\n",
        "  raise ValueError(f'Label {target_label} not found in labels: {labels}')\n",
        "\n",
        "\n",
        "def get_label_position_x(\n",
        "    target_label: str,\n",
        "    ax: plt.Axes,\n",
        ") -\u003e int:\n",
        "  \"\"\"Returns the position of the target x label in the given axis.\"\"\"\n",
        "  return _get_label_position(ax.xaxis.get_ticklabels(), target_label)\n",
        "\n",
        "\n",
        "def get_label_position_y(\n",
        "    target_label: str,\n",
        "    ax: plt.Axes,\n",
        ") -\u003e int:\n",
        "  \"\"\"Returns the position of the target y label in the given axis.\"\"\"\n",
        "  return _get_label_position(ax.yaxis.get_ticklabels(), target_label)\n",
        "\n",
        "\n",
        "def get_bar_x_coord(\n",
        "    label: str,\n",
        "    label_position: int,\n",
        "    ax: plt.Axes,\n",
        ") -\u003e int:\n",
        "  \"\"\"Returns the x coordinate of the given bar in the given axis.\"\"\"\n",
        "  labels = [x.get_text() for x in ax.get_legend().get_texts()]\n",
        "  bars = [i for i in ax.containers if isinstance(i, container.BarContainer)]\n",
        "  assert len(labels) == len(bars)\n",
        "  for bar_label, bar in zip(labels, bars):\n",
        "    if bar_label == label:\n",
        "      bar = bar.patches[label_position]\n",
        "      return bar.get_x() + (bar.get_width() / 2)\n",
        "  raise ValueError(f'Label {label} not found in axis: {ax}')\n",
        "\n",
        "\n",
        "def get_bar_mid_x_coords(\n",
        "    pair: tuple[str, str],\n",
        "    label_position: int,\n",
        "    ax: plt.Axes,\n",
        ") -\u003e tuple[int, int]:\n",
        "  \"\"\"Returns the mid x coordinates of each bar pair in the given axis.\"\"\"\n",
        "  labels = [x.get_text() for x in ax.get_legend().get_texts()]\n",
        "  bars = [i for i in ax.containers if isinstance(i, container.BarContainer)]\n",
        "  assert len(labels) == len(bars)\n",
        "  x_coords = []\n",
        "  for label, bar in zip(labels, bars):\n",
        "    if label in pair:\n",
        "      bar = bar.patches[label_position]\n",
        "      x_coords.append(bar.get_x() + (bar.get_width() / 2))\n",
        "  assert len(x_coords) == 2, (pair, label_position, x_coords)\n",
        "  return tuple(sorted(x_coords))\n",
        "\n",
        "\n",
        "def get_bar_mid_y_coords(\n",
        "    pair: tuple[str, str],\n",
        "    label_position: int,\n",
        "    ax: plt.Axes,\n",
        ") -\u003e tuple[int, int]:\n",
        "  \"\"\"Returns the mid y coordinates of each bar pair in the given axis.\"\"\"\n",
        "  labels = [x.get_text() for x in ax.get_legend().get_texts()]\n",
        "  bars = [i for i in ax.containers if isinstance(i, container.BarContainer)]\n",
        "  assert len(labels) == len(bars)\n",
        "  y_coords = []\n",
        "  for label, bar in zip(labels, bars):\n",
        "    if label in pair:\n",
        "      bar = bar.patches[label_position]\n",
        "      y_coords.append(bar.get_y() + (bar.get_height() / 2))\n",
        "  assert len(y_coords) == 2, y_coords\n",
        "  return tuple(sorted(y_coords))\n",
        "\n",
        "\n",
        "def add_significance_x(\n",
        "    ax: plt.Axes,\n",
        "    x1: float,\n",
        "    x2: float,\n",
        "    y: float,\n",
        "    h: float,\n",
        "    text: str = '*',\n",
        ") -\u003e None:\n",
        "  \"\"\"Add significance annotations between two bar midpoints on the x axis.\n",
        "\n",
        "  Args:\n",
        "    ax: The axis to annotate.\n",
        "    x1: The x midpoint coordinate of the first bar.\n",
        "    x2: The x midpoint coordinate of the second bar.\n",
        "    y: The y coordinate at which to start the annotation bracket.\n",
        "    h: The height of the annotation bracket.\n",
        "    text: The text in the annotation.\n",
        "  \"\"\"\n",
        "  ax.plot([x1, x1, x2, x2], [y, y + h, y + h, y], lw=2, c='k')\n",
        "  ax.text(\n",
        "      (x1 + x2) * 0.5,\n",
        "      y + h,\n",
        "      text,\n",
        "      ha='center',\n",
        "      va='bottom',\n",
        "      color='k',\n",
        "      fontdict={'family': 'monospace'},\n",
        "  )\n",
        "\n",
        "\n",
        "def add_significance_y(\n",
        "    ax: plt.Axes,\n",
        "    y1: float,\n",
        "    y2: float,\n",
        "    x: float,\n",
        "    d: float,\n",
        "    text: str = '*',\n",
        ") -\u003e None:\n",
        "  \"\"\"Add significance annotations between two bar midpoints on the y axis.\n",
        "\n",
        "  Args:\n",
        "    ax: The axis to annotate.\n",
        "    y1: The y midpoint coordinate of the first bar.\n",
        "    y2: The y midpoint coordinate of the second bar.\n",
        "    x: The x coordinate at which to start the annotation bracket.\n",
        "    d: The depth of the annotation bracket.\n",
        "    text: The text in the annotation.\n",
        "  \"\"\"\n",
        "  ax.plot([x, x + d, x + d, x], [y1, y1, y2, y2], lw=2, c='k')\n",
        "  ax.text(\n",
        "      x + (2 * d),\n",
        "      # Accounting for lineheight, which puts the * a bit too high.\n",
        "      ((y1 + y2) * 0.5) - 0.1,\n",
        "      text,\n",
        "      ha='center',\n",
        "      va='center',\n",
        "      color='k',\n",
        "      fontdict={'family': 'monospace'},\n",
        "  )\n",
        "\n",
        "\n",
        "def _set_axis_attrs_section(\n",
        "    ax: plt.Axes,\n",
        "    title: str,\n",
        "    xlabel: str,\n",
        "    ylabel: str,\n",
        "    fontsize: int,\n",
        ") -\u003e None:\n",
        "  \"\"\"Sets x and y axis attributes for a \"Section\" plot.\"\"\"\n",
        "  ax.set_title(title, weight='bold', fontsize=fontsize)\n",
        "  ax.set_ylabel(ylabel, fontsize=fontsize)\n",
        "  ax.set_ylim((1, 5.5))\n",
        "  ax.tick_params(bottom=False, left=True, width=1.5, direction='inout')\n",
        "  ax.set_yticks(ticks=list(range(1, 6, 1)))\n",
        "  ax.set_yticklabels(labels=list(range(1, 6, 1)), fontsize=fontsize)\n",
        "  ax.set_xlabel(xlabel, weight='bold', fontsize=fontsize)\n",
        "  labels = [\n",
        "      SECTION_TAG_TO_LABEL.get(l.get_text(), l.get_text())\n",
        "      for l in ax.get_xticklabels()\n",
        "  ]\n",
        "  ax.set_xticklabels(labels=labels, fontsize=fontsize)\n",
        "\n",
        "\n",
        "def _set_axis_attrs_principle(\n",
        "    ax: plt.Axes,\n",
        "    title: str,\n",
        "    xlabel: str,\n",
        "    ylabel: str,\n",
        "    title_fontsize: int,\n",
        "    other_fontsize: int,\n",
        "    label_rotation_x: int = 0,\n",
        "    label_rotation_y: int = 0,\n",
        ") -\u003e None:\n",
        "  \"\"\"Sets x and y axis attributes for a \"Principle\" plot.\"\"\"\n",
        "  ax.set_title(title, weight='bold', fontsize=title_fontsize)\n",
        "  ax.set_xlabel(xlabel, fontsize=title_fontsize)\n",
        "  ax.set_ylabel(ylabel, weight='bold', fontsize=title_fontsize)\n",
        "  ax.set_xlim((1, 5.5))\n",
        "  ax.set_ylim((-1.15, 15.15))\n",
        "  ax.tick_params(bottom=True, left=False, width=1.5, direction='inout')\n",
        "  if xlabel:\n",
        "    ax.set_xticks(ticks=list(range(1, 6, 1)))\n",
        "    ax.set_xticklabels(\n",
        "        labels=list(range(1, 6, 1)),\n",
        "        fontsize=other_fontsize,\n",
        "        rotation=label_rotation_x,\n",
        "    )\n",
        "  if ylabel:\n",
        "    labels = [\n",
        "        PRINCIPLE_TO_LABEL.get(l.get_text(), l.get_text())\n",
        "        for l in ax.get_yticklabels()\n",
        "    ]\n",
        "    ax.set_yticklabels(\n",
        "        labels=labels,\n",
        "        fontsize=other_fontsize,\n",
        "        rotation=label_rotation_y,\n",
        "    )\n",
        "\n",
        "\n",
        "def _label_axes(fig: figure.Figure) -\u003e None:\n",
        "  \"\"\"Adds alpha sublabels (e.g., a, b, ...) to each axis in a figure.\"\"\"\n",
        "  labeled_axes = [*fig.get_axes()]\n",
        "  for i, ax in enumerate(labeled_axes):\n",
        "    ax_label = f'{string.ascii_lowercase[i]}'\n",
        "    trans = transforms.ScaledTranslation(-20 / 72, 7 / 72, fig.dpi_scale_trans)\n",
        "    ax.text(\n",
        "        -0.04,\n",
        "        1.0,\n",
        "        ax_label,\n",
        "        transform=ax.transAxes + trans,\n",
        "        fontsize='18',\n",
        "        va='bottom',\n",
        "        weight='bold',\n",
        "    )\n",
        "\n",
        "\n",
        "def _plot_by_section_to_ax(\n",
        "    ax: plt.Axes,\n",
        "    df: pd.DataFrame,\n",
        "    order: list[str] | None,\n",
        "    hue_order: list[str] | None,\n",
        "    rating_col: str = COL_RATING,\n",
        "    add_stat_sig_annot: bool = False,\n",
        "    pairwise_statsig: bool = True,\n",
        ") -\u003e None:\n",
        "  \"\"\"Plots ratings by section to the given axis.\"\"\"\n",
        "  sns.barplot(\n",
        "      ax=ax,\n",
        "      x=COL_SECTION_TAG,\n",
        "      y=rating_col,\n",
        "      hue=COL_CONVERSATION_SOURCE,\n",
        "      data=df,\n",
        "      errorbar=('ci', 95),\n",
        "      n_boot=1_000,\n",
        "      palette=dict(CONVERSATION_SOURCE_PALETTE),\n",
        "      order=order,\n",
        "      hue_order=hue_order,\n",
        "      # Uncomment to add spacing between bars, but this requires fixing the\n",
        "      # legend (which also has the line width applied).\n",
        "      # linewidth=5,\n",
        "      # edgecolor='white',\n",
        "  )\n",
        "\n",
        "  if add_stat_sig_annot:\n",
        "    sig_df = significance_test(\n",
        "        df=df,\n",
        "        groupby_column=COL_SECTION_TAG,\n",
        "        rating_column=rating_col,\n",
        "    )\n",
        "    if pairwise_statsig:\n",
        "      for sig_section, pairs in get_sig_sections(sig_df).items():\n",
        "        label_position = get_label_position_x(sig_section, ax)\n",
        "        for pair in pairs:\n",
        "          if pair[0] not in hue_order or pair[1] not in hue_order:\n",
        "            continue\n",
        "          x_coords = get_bar_mid_x_coords(pair, label_position, ax)\n",
        "          add_significance_x(ax, x_coords[0], x_coords[1], 5.1, 0.1, '*')\n",
        "    else:\n",
        "      mean_by_section = (\n",
        "          df.groupby([COL_SECTION_TAG, COL_CONVERSATION_SOURCE])[rating_col]\n",
        "          .mean()\n",
        "          .reset_index()\n",
        "          .sort_values(by=rating_col, ascending=False)\n",
        "      )\n",
        "      highest_per_section = (\n",
        "          mean_by_section.groupby(COL_SECTION_TAG).first().reset_index()\n",
        "      )\n",
        "      section_to_highest_type = dict(\n",
        "          zip(\n",
        "              highest_per_section[COL_SECTION_TAG],\n",
        "              highest_per_section[COL_CONVERSATION_SOURCE],\n",
        "          )\n",
        "      )\n",
        "      # For each pair of non-stat-sig-diff sources in a section, add a \"*\" to\n",
        "      # each source that is non-stat-sig-diff from the *highest* source within\n",
        "      # that section.\n",
        "      for non_sig_section, pairs in get_non_sig_sections(sig_df).items():\n",
        "        label_position = get_label_position_x(non_sig_section, ax)\n",
        "        highest_section = section_to_highest_type[non_sig_section]\n",
        "        ax.text(\n",
        "            get_bar_x_coord(highest_section, label_position, ax),\n",
        "            5.1 + 0.1,\n",
        "            '*',\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            color='k',\n",
        "            fontdict={'family': 'monospace'},\n",
        "        )\n",
        "        starred_labels = set({highest_section})\n",
        "        for pair in pairs:\n",
        "          if highest_section not in pair:\n",
        "            continue\n",
        "          for pair_element in pair:\n",
        "            if pair_element in starred_labels:\n",
        "              continue\n",
        "            x_coord = get_bar_x_coord(pair_element, label_position, ax)\n",
        "            ax.text(\n",
        "                x_coord,\n",
        "                5.1 + 0.1,\n",
        "                '*',\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                color='k',\n",
        "                fontdict={'family': 'monospace'},\n",
        "            )\n",
        "            starred_labels.add(pair_element)\n",
        "\n",
        "\n",
        "def plot_case_study_main_fig(\n",
        "    sleep_df: pd.DataFrame,\n",
        "    fitness_df: pd.DataFrame,\n",
        "    sleep_title: str,\n",
        "    fitness_title: str,\n",
        "    savefig_filepath: str,\n",
        "    hue_order: list[str],\n",
        "    savefig_format: str = 'pdf',\n",
        "    sleep_rating_col: str = COL_RATING,\n",
        "    sleep_rating_label: str = LABEL_AVG_RATING,\n",
        "    fitness_rating_col: str = COL_RATING,\n",
        "    fitness_rating_label: str = LABEL_AVG_RATING,\n",
        "    other_font_size: int = 16,\n",
        "    label_subplots: bool = False,\n",
        "    pairwise_statsig: bool = True,  # If false, we star the best and any _not_ statsig from best.\n",
        "    drop_overall: bool = True,\n",
        ") -\u003e None:\n",
        "  \"\"\"Plots ratings by section for both verticals side-by-side.\"\"\"\n",
        "  # Drop the overall section.\n",
        "  if drop_overall:\n",
        "    sleep_df = sleep_df[sleep_df[COL_SECTION_TAG] != 'overall'].reset_index(\n",
        "        drop=True\n",
        "    )\n",
        "    fitness_df = fitness_df[\n",
        "        fitness_df[COL_SECTION_TAG] != 'overall'\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "  sleep_df = sleep_df[\n",
        "      sleep_df[COL_CONVERSATION_SOURCE].isin(hue_order)\n",
        "  ].reset_index(drop=True)\n",
        "  fitness_df = fitness_df[\n",
        "      fitness_df[COL_CONVERSATION_SOURCE].isin(hue_order)\n",
        "  ].reset_index(drop=True)\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "  _plot_by_section_to_ax(\n",
        "      ax=axes[0],\n",
        "      df=sleep_df,\n",
        "      order=SLEEP_SECTION_TAG_ORDER,\n",
        "      hue_order=hue_order,\n",
        "      rating_col=sleep_rating_col,\n",
        "      add_stat_sig_annot=True,\n",
        "      pairwise_statsig=pairwise_statsig,\n",
        "  )\n",
        "  _plot_by_section_to_ax(\n",
        "      ax=axes[1],\n",
        "      df=fitness_df,\n",
        "      order=FITNESS_SECTION_TAG_ORDER,\n",
        "      hue_order=hue_order,\n",
        "      rating_col=fitness_rating_col,\n",
        "      add_stat_sig_annot=True,\n",
        "      pairwise_statsig=pairwise_statsig,\n",
        "  )\n",
        "\n",
        "  # Add a legend to the first subplot.\n",
        "  legend_handles = axes[0].get_legend().legend_handles\n",
        "  legend_labels = [\n",
        "      CONVERSATION_SOURCE_KEY_TO_LABEL.get(t.get_text(), t.get_text())\n",
        "      for t in axes[0].get_legend().get_texts()\n",
        "  ]\n",
        "  axes[0].legend(\n",
        "      loc='lower left',\n",
        "      frameon=True,\n",
        "      facecolor='white',\n",
        "      edgecolor='white',\n",
        "      framealpha=1,\n",
        "      handlelength=0.7,\n",
        "      borderpad=0.3,\n",
        "      bbox_to_anchor=(0.065, 0),\n",
        "      handles=legend_handles,\n",
        "      labels=legend_labels,\n",
        "  )\n",
        "  axes[1].get_legend().remove()\n",
        "\n",
        "  # Update x and y axis attributes.\n",
        "  _set_axis_attrs_section(\n",
        "      axes[0],\n",
        "      title=sleep_title,\n",
        "      xlabel=LABEL_SECTION,\n",
        "      ylabel=sleep_rating_label,\n",
        "      fontsize=other_font_size,\n",
        "  )\n",
        "  _set_axis_attrs_section(\n",
        "      axes[1],\n",
        "      title=fitness_title,\n",
        "      xlabel=LABEL_SECTION,\n",
        "      ylabel=fitness_rating_label,\n",
        "      fontsize=other_font_size,\n",
        "  )\n",
        "\n",
        "  if label_subplots:\n",
        "    _label_axes(fig)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.savefig(savefig_filepath, format=savefig_format, dpi=300)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def _add_stat_sig_marker_principle(\n",
        "    df: pd.DataFrame,\n",
        "    rating_col: str,\n",
        "    ax: plt.Axes,\n",
        ") -\u003e None:\n",
        "  sig_df = significance_test(df, COL_PRINCIPLE, rating_col)\n",
        "  for sig_section, pairs in get_sig_sections(sig_df).items():\n",
        "    label_position = get_label_position_y(sig_section, ax)\n",
        "    for pair in pairs:\n",
        "      y_coords = get_bar_mid_y_coords(pair, label_position, ax)\n",
        "      add_significance_y(ax, y_coords[0], y_coords[1], 5.1, 0.1, '*')\n",
        "\n",
        "\n",
        "def plot_by_principle_all(\n",
        "    sleep_df: pd.DataFrame,\n",
        "    sleep_title: str,\n",
        "    fitness_df: pd.DataFrame,\n",
        "    fitness_title: str,\n",
        "    savefig_filepath: str,\n",
        "    order: list[str] | None,\n",
        "    hue_order: list[str] | None,\n",
        "    sleep_rating_col: str = COL_RATING,\n",
        "    fitness_rating_col: str = COL_RATING,\n",
        "    title_font_size: int = 16,\n",
        "    other_font_size: int = 8,\n",
        "    add_stat_sig_annot: bool = False,\n",
        "    label_subplots: bool = False,\n",
        ") -\u003e None:\n",
        "  \"\"\"Plots all ratings grouped by conversation source and principle.\"\"\"\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(16, 10), sharey=True)\n",
        "  sns.barplot(\n",
        "      y=COL_PRINCIPLE,\n",
        "      x=sleep_rating_col,\n",
        "      hue=COL_CONVERSATION_SOURCE,\n",
        "      data=sleep_df,\n",
        "      errorbar=('ci', 95),\n",
        "      n_boot=1_000,\n",
        "      palette=dict(CONVERSATION_SOURCE_PALETTE),\n",
        "      # Reverse the order since seaborn plots the last in list on top.\n",
        "      order=order[::-1],\n",
        "      hue_order=hue_order,\n",
        "      orient='h',\n",
        "      ax=axes[0],\n",
        "  )\n",
        "  sns.barplot(\n",
        "      y=COL_PRINCIPLE,\n",
        "      x=fitness_rating_col,\n",
        "      hue=COL_CONVERSATION_SOURCE,\n",
        "      data=fitness_df,\n",
        "      errorbar=('ci', 95),\n",
        "      n_boot=1_000,\n",
        "      palette=dict(CONVERSATION_SOURCE_PALETTE),\n",
        "      # Reverse the order since seaborn plots the last in list on top.\n",
        "      order=order[::-1],\n",
        "      hue_order=hue_order,\n",
        "      orient='h',\n",
        "      ax=axes[1],\n",
        "  )\n",
        "  if add_stat_sig_annot:\n",
        "    _add_stat_sig_marker_principle(sleep_df, sleep_rating_col, axes[0])\n",
        "    _add_stat_sig_marker_principle(fitness_df, fitness_rating_col, axes[1])\n",
        "\n",
        "  # Add a legend to the first subplot.\n",
        "  legend_handles = axes[0].get_legend().legend_handles\n",
        "  legend_labels = [\n",
        "      CONVERSATION_SOURCE_KEY_TO_LABEL.get(t.get_text(), t.get_text())\n",
        "      for t in axes[0].get_legend().get_texts()\n",
        "  ]\n",
        "  axes[0].legend(\n",
        "      loc='upper left',\n",
        "      frameon=True,\n",
        "      facecolor='white',\n",
        "      edgecolor='white',\n",
        "      framealpha=1,\n",
        "      handlelength=0.7,\n",
        "      borderpad=0.3,\n",
        "      bbox_to_anchor=(0, 1 - 0.046),\n",
        "      reverse=True,\n",
        "      handles=legend_handles,\n",
        "      labels=legend_labels,\n",
        "  )\n",
        "  axes[1].get_legend().remove()\n",
        "\n",
        "  # Update x and y axis attributes.\n",
        "  _set_axis_attrs_principle(\n",
        "      axes[0],\n",
        "      sleep_title,\n",
        "      xlabel=LABEL_AVG_RATING,\n",
        "      ylabel=LABEL_PRINCIPLE,\n",
        "      title_fontsize=title_font_size,\n",
        "      other_fontsize=other_font_size,\n",
        "      label_rotation_y=45,\n",
        "  )\n",
        "  _set_axis_attrs_principle(\n",
        "      axes[1],\n",
        "      fitness_title,\n",
        "      xlabel=LABEL_AVG_RATING,\n",
        "      ylabel='',\n",
        "      title_fontsize=title_font_size,\n",
        "      other_fontsize=other_font_size,\n",
        "  )\n",
        "\n",
        "  if label_subplots:\n",
        "    _label_axes(fig)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  fig.savefig(savefig_filepath, format='pdf', dpi=300)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def _rater_id_to_label(rater_id: str) -\u003e str:\n",
        "  \"\"\"Returns the rater label for a given a rater ID.\"\"\"\n",
        "  return ' '.join(x.capitalize() for x in rater_id.split('_'))\n",
        "\n",
        "\n",
        "def _rater_id_to_type(rater_id: str) -\u003e RaterType:\n",
        "  \"\"\"Returns the rater type for a given a rater ID.\"\"\"\n",
        "  if RaterType.PRIMARY.value in rater_id:\n",
        "    return RaterType.PRIMARY\n",
        "  elif RaterType.SECONDARY.value in rater_id:\n",
        "    return RaterType.SECONDARY\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected {rater_id=}')\n",
        "\n",
        "\n",
        "def _key_column(df_row: dict[str, Any]) -\u003e str:\n",
        "  \"\"\"Returns a unique key column value for the given expert rating row.\"\"\"\n",
        "  case_study_id = df_row[COL_CASE_STUDY_ID]\n",
        "  rater = df_row[COL_RATER]\n",
        "  tag = df_row[COL_SECTION_TAG]\n",
        "  principle = df_row[COL_PRINCIPLE]\n",
        "  conversation_source = df_row[COL_CONVERSATION_SOURCE]\n",
        "  key = f'{case_study_id}::{rater}::{tag}::{principle}::{conversation_source}'\n",
        "  return key\n",
        "\n",
        "\n",
        "def _key_no_rater_column(df_row: dict[str, Any]) -\u003e str:\n",
        "  \"\"\"Returns a key column value for the given expert rating row sans rater.\"\"\"\n",
        "  case_study_id = df_row[COL_CASE_STUDY_ID]\n",
        "  tag = df_row[COL_SECTION_TAG]\n",
        "  principle = df_row[COL_PRINCIPLE]\n",
        "  conversation_source = df_row[COL_CONVERSATION_SOURCE]\n",
        "  key_no_rater = f'{case_study_id}::{tag}::{principle}::{conversation_source}'\n",
        "  return key_no_rater\n",
        "\n",
        "\n",
        "def get_rater_pairs(df: pd.DataFrame) -\u003e set[tuple[str, str]]:\n",
        "  \"\"\"Returns the set of rater pairs for which we have replicated ratings.\"\"\"\n",
        "  rater_groups = set(\n",
        "      df.groupby(by=DERIVED_COL_KEY_NO_RATER)\n",
        "      .rater.agg(lambda x: tuple(sorted(x)))\n",
        "      .values\n",
        "  )\n",
        "  rater_pairs = set()\n",
        "  for rater_group in rater_groups:\n",
        "    if len(rater_group) \u003c 2:\n",
        "      continue\n",
        "    combinations = list(itertools.combinations(rater_group, 2))\n",
        "    for combination in combinations:\n",
        "      rater_pairs.add((combination[0], combination[1]))\n",
        "  return rater_pairs\n",
        "\n",
        "\n",
        "def plot_rating_contingency(\n",
        "    df: pd.DataFrame,\n",
        "    col_a: str,\n",
        "    label_a: str,\n",
        "    col_b: str,\n",
        "    label_b: str,\n",
        "    label_values: list[str] = [1, 2, 3, 4, 5],\n",
        "    plot_percents: bool = False,\n",
        "    plot_cbar: bool = False,\n",
        "    ax: plt.Axes | None = None,\n",
        "    color: str = COLOR_GRAY_DARK,\n",
        ") -\u003e plt.Axes:\n",
        "  \"\"\"Plots a contingency table for samples where both ratings are present.\n",
        "\n",
        "  Args:\n",
        "    df: A dataframe containing the target columns.\n",
        "    col_a: The column containing values for the vertical axis.\n",
        "    label_a: `col_a`'s label value.\n",
        "    col_b: The column containing values for the horizontal axis.\n",
        "    label_b: `col_b`'s label value.\n",
        "    label_values: The possible label values.\n",
        "    plot_percents: Whether to plot percentages of the contingency table.\n",
        "    plot_cbar: Whether to include the cbar to the right of the contingency\n",
        "      table.\n",
        "    ax: An optional axis on which the spirogram is plotted; if not specified, a\n",
        "      new axis is created.\n",
        "\n",
        "  Returns:\n",
        "    The axis on which the contingency table was plotted.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    ax = plt.axes()\n",
        "\n",
        "  # Get agreement values.\n",
        "  contingency_df = df[[col_a, col_b]].dropna().copy()\n",
        "  label_a_values = contingency_df[col_a].values\n",
        "  label_b_values = contingency_df[col_b].values\n",
        "  cf_matrix = sklearn.metrics.confusion_matrix(\n",
        "      label_a_values,\n",
        "      label_b_values,\n",
        "      labels=label_values,\n",
        "  )\n",
        "  group_counts = [f'{value:g}' for value in cf_matrix.flatten()]\n",
        "  group_percentages = [\n",
        "      f'{value:0.2%}' for value in cf_matrix.flatten() / np.sum(cf_matrix)\n",
        "  ]\n",
        "  if plot_percents:\n",
        "    labels = [\n",
        "        f'{v2}\\n({v3})' for v2, v3 in zip(group_counts, group_percentages)\n",
        "    ]\n",
        "  else:\n",
        "    labels = group_counts\n",
        "  labels = np.asarray(labels).reshape(5, 5)\n",
        "  sns.heatmap(\n",
        "      cf_matrix,\n",
        "      square=True,\n",
        "      cmap=sns.light_palette(color, as_cmap=True),\n",
        "      fmt='',\n",
        "      annot=labels,\n",
        "      xticklabels=label_values,\n",
        "      yticklabels=label_values,\n",
        "      cbar=plot_cbar,\n",
        "      ax=ax,\n",
        "  )\n",
        "  ax.set_yticklabels(labels=ax.get_yticklabels(), va='center')\n",
        "  ax.set_ylabel(label_a)\n",
        "  ax.set_xlabel(label_b)\n",
        "  return ax\n",
        "\n",
        "\n",
        "def _process_vertical_pairs(\n",
        "    df: pd.DataFrame,\n",
        "    rater_types_a: list[RaterType],\n",
        "    rater_types_b: list[RaterType],\n",
        ") -\u003e tuple[list[str], list[tuple[str, str]], list[list[tuple[str, str]]]]:\n",
        "  \"\"\"Returns a tuple of colors, rater pairs, and grouped rater pairs.\"\"\"\n",
        "  colors = []\n",
        "  all_filtered_pairs = []\n",
        "  filtered_pair_groups = []\n",
        "  for rater_type_a, rater_type_b in zip(rater_types_a, rater_types_b):\n",
        "    if rater_type_a == RaterType.PRIMARY and rater_type_b == RaterType.PRIMARY:\n",
        "      colors.append(COLOR_BLUE)\n",
        "    elif (\n",
        "        rater_type_a == RaterType.PRIMARY\n",
        "        and rater_type_b == RaterType.SECONDARY\n",
        "    ):\n",
        "      colors.append(COLOR_GREEN)\n",
        "    elif (\n",
        "        rater_type_a == RaterType.SECONDARY\n",
        "        and rater_type_b == RaterType.SECONDARY\n",
        "    ):\n",
        "      colors.append(COLOR_YELLOW)\n",
        "    else:\n",
        "      raise ValueError('Unknown rater type pairing.')\n",
        "    target_rater_pair_type = tuple(\n",
        "        sorted([rater_type_a.value, rater_type_b.value])\n",
        "    )\n",
        "    rater_pairs = get_rater_pairs(df)\n",
        "    filtered_pairs = [\n",
        "        pair\n",
        "        for pair in rater_pairs\n",
        "        if target_rater_pair_type\n",
        "        == tuple(\n",
        "            sorted([\n",
        "                _rater_id_to_type(pair[0]).value,\n",
        "                _rater_id_to_type(pair[1]).value,\n",
        "            ])\n",
        "        )\n",
        "    ]\n",
        "    all_filtered_pairs.extend(filtered_pairs)\n",
        "    filtered_pair_groups.append(filtered_pairs)\n",
        "  return colors, all_filtered_pairs, filtered_pair_groups\n",
        "\n",
        "\n",
        "def plot_rater_contingency_all_both_verticals(\n",
        "    sleep_df: pd.DataFrame,\n",
        "    fitness_df: pd.DataFrame,\n",
        "    rater_types_a: list[RaterType],\n",
        "    rater_types_b: list[RaterType],\n",
        "    filename: str | None = None,\n",
        ") -\u003e pd.DataFrame:\n",
        "  if len(rater_types_a) != len(rater_types_b):\n",
        "    raise ValueError('rater_type_a and rater_type_b must have the same length.')\n",
        "\n",
        "  sleep_df = sleep_df.copy()\n",
        "  fitness_df = fitness_df.copy()\n",
        "\n",
        "  sleep_df[DERIVED_COL_KEY_NO_RATER] = sleep_df.apply(\n",
        "      _key_no_rater_column, axis=1\n",
        "  )\n",
        "  fitness_df[DERIVED_COL_KEY_NO_RATER] = fitness_df.apply(\n",
        "      _key_no_rater_column, axis=1\n",
        "  )\n",
        "\n",
        "  sleep_colors, sleep_all_filtered_pairs, sleep_filtered_pair_groups = (\n",
        "      _process_vertical_pairs(sleep_df, rater_types_a, rater_types_b)\n",
        "  )\n",
        "  (\n",
        "      readiness_colors,\n",
        "      readiness_all_filtered_pairs,\n",
        "      readiness_filtered_pair_groups,\n",
        "  ) = _process_vertical_pairs(fitness_df, rater_types_a, rater_types_b)\n",
        "\n",
        "  assert len(sleep_all_filtered_pairs) == 12\n",
        "  assert len(readiness_all_filtered_pairs) == 11\n",
        "\n",
        "  sleep_df_pivot = sleep_df.pivot(\n",
        "      index=DERIVED_COL_KEY_NO_RATER,\n",
        "      columns=COL_RATER,\n",
        "      values=COL_RATING,\n",
        "  )\n",
        "  fitness_df_pivot = fitness_df.pivot(\n",
        "      index=DERIVED_COL_KEY_NO_RATER,\n",
        "      columns=COL_RATER,\n",
        "      values=COL_RATING,\n",
        "  )\n",
        "  num_cols = 4\n",
        "  num_rows = 6\n",
        "  fig, axes = plt.subplots(\n",
        "      num_rows,\n",
        "      num_cols + 1,\n",
        "      figsize=(4 * num_cols, 3.5 * num_rows),\n",
        "      dpi=300,\n",
        "      gridspec_kw={'width_ratios': [1, 1, 0.1, 1, 1]},\n",
        "  )\n",
        "  # Handle matplotlib returning varied types/shapes for subplots.\n",
        "  if not isinstance(axes, np.ndarray):\n",
        "    axes = [[axes]]\n",
        "  elif len(axes.shape) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  for offset, df_pivot, colors, filtered_pair_groups in [\n",
        "      (0, sleep_df_pivot, sleep_colors, sleep_filtered_pair_groups),\n",
        "      (\n",
        "          num_cols // 2 + 1,\n",
        "          fitness_df_pivot,\n",
        "          readiness_colors,\n",
        "          readiness_filtered_pair_groups,\n",
        "      ),\n",
        "  ]:\n",
        "    num_seen_in_other_groups = 0\n",
        "    for k, filtered_pair_group in enumerate(filtered_pair_groups):\n",
        "      for n, (rater_a, rater_b) in enumerate(\n",
        "          sorted(\n",
        "              filtered_pair_group,\n",
        "              key=lambda x: (\n",
        "                  min([_rater_id_to_label(x[0]), _rater_id_to_label(x[1])]),\n",
        "                  max(\n",
        "                      [_rater_id_to_label(x[0]), _rater_id_to_label(x[1])]\n",
        "                  ),  # For tie breaks.\n",
        "              ),\n",
        "          )\n",
        "      ):\n",
        "        n = n + num_seen_in_other_groups\n",
        "        i = n // (num_cols // 2)\n",
        "        j = n % (num_cols // 2)\n",
        "        j += offset\n",
        "        label_a = _rater_id_to_label(rater_a)\n",
        "        label_b = _rater_id_to_label(rater_b)\n",
        "        # Nature of labels should allow us to alpha order so we are consistent\n",
        "        # with axes across pairs.\n",
        "        if label_a \u003c label_b:\n",
        "          rater_a, rater_b = rater_b, rater_a\n",
        "          label_a, label_b = label_b, label_a\n",
        "        rater_pair_df = (\n",
        "            df_pivot[[\n",
        "                rater_a,\n",
        "                rater_b,\n",
        "            ]]\n",
        "            .dropna()\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "        plot_rating_contingency(\n",
        "            rater_pair_df,\n",
        "            col_a=rater_a,\n",
        "            label_a=label_a,\n",
        "            col_b=rater_b,\n",
        "            label_b=label_b,\n",
        "            ax=axes[i][j],\n",
        "            color=colors[k],\n",
        "        )\n",
        "      num_seen_in_other_groups += len(filtered_pair_group)\n",
        "\n",
        "  # Hide unused axes.\n",
        "  for i in range(num_rows):\n",
        "    axes[i][2].set_visible(False)\n",
        "  axes[num_rows - 1][num_cols].set_visible(False)\n",
        "\n",
        "  # Add a, b, etc. labels to subfigures.\n",
        "  labeled_axes = [axes[0][0], axes[0][3]]\n",
        "  for i, ax in enumerate(labeled_axes):  # Skip the legend.\n",
        "    ax_label = f'{string.ascii_lowercase[i]}'\n",
        "    trans = transforms.ScaledTranslation(-20 / 72, 7 / 72, fig.dpi_scale_trans)\n",
        "    ax.text(\n",
        "        -0.04,\n",
        "        1.0,\n",
        "        ax_label,\n",
        "        transform=ax.transAxes + trans,\n",
        "        fontsize='18',\n",
        "        va='bottom',\n",
        "        weight='bold',\n",
        "    )\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "  if filename:\n",
        "    fig.savefig(filename, format='pdf', dpi=300)\n",
        "\n",
        "\n",
        "def _plot_stats_for_metric_binary(\n",
        "    df: pd.DataFrame,\n",
        "    metric_name: str,\n",
        "    ax: plt.Axes,\n",
        "    plot_cmap: bool = False,\n",
        "    label_rotation_x: int = 90,\n",
        "    label_rotation_y: int = 0,\n",
        "    title_prefix: str | None = None,\n",
        ") -\u003e None:\n",
        "  rater_to_label = {\n",
        "      r: _rater_id_to_label(r)\n",
        "      for r in list(df.model_name.unique()) + list(df.prediction_name.unique())\n",
        "  }\n",
        "  label_to_rater = {l: r for r, l in rater_to_label.items()}\n",
        "  rater_to_label = {\n",
        "      k: v.replace('Sleep ', '').replace('Fitness ', '')\n",
        "      for k, v in rater_to_label.items()\n",
        "  }\n",
        "  label_to_rater = {\n",
        "      k.replace('Sleep ', '').replace('Fitness ', ''): v\n",
        "      for k, v in label_to_rater.items()\n",
        "  }\n",
        "  df = df.copy()\n",
        "  df = df.replace(rater_to_label)\n",
        "  df = df.replace(label_to_rater)\n",
        "  sub_df = (\n",
        "      df[df.metric_name == metric_name]\n",
        "      .reset_index(drop=True)\n",
        "      .rename(columns={'model_name': 'outer', 'prediction_name': 'inner'})\n",
        "      .replace(rater_to_label)\n",
        "  )\n",
        "  reflection_df = sub_df.copy()\n",
        "  reflection_df['inner'], reflection_df['outer'] = (\n",
        "      reflection_df.outer,\n",
        "      reflection_df.inner,\n",
        "  )\n",
        "  sub_df = pd.concat([sub_df, reflection_df]).reset_index(drop=True)\n",
        "  sub_df_pivot = (\n",
        "      sub_df.pivot(\n",
        "          index='outer',\n",
        "          columns='inner',\n",
        "          values='mean',\n",
        "      )\n",
        "      .sort_index()\n",
        "      .sort_index(axis=1)\n",
        "  )\n",
        "  sub_df_pivot = sub_df_pivot.sort_index().sort_index(axis=1)\n",
        "  annot_labels = []\n",
        "  for outer in sub_df_pivot.index:\n",
        "    dim_labels = []\n",
        "    for inner in sub_df_pivot.columns:\n",
        "      records = sub_df[\n",
        "          (sub_df.inner == inner) \u0026 (sub_df.outer == outer)\n",
        "      ].to_dict('records')\n",
        "      if len(records) == 0:\n",
        "        label = ''\n",
        "      else:\n",
        "        record = records[0]\n",
        "        sample_records = df[\n",
        "            (df.model_name == label_to_rater[outer])\n",
        "            \u0026 (df.prediction_name == label_to_rater[inner])\n",
        "            \u0026 (df.metric_name == 'sample_count')\n",
        "        ].to_dict('records')\n",
        "        if len(sample_records) == 0:\n",
        "          # Handle reflection.\n",
        "          sample_records = df[\n",
        "              (df.model_name == label_to_rater[inner])\n",
        "              \u0026 (df.prediction_name == label_to_rater[outer])\n",
        "              \u0026 (df.metric_name == 'sample_count')\n",
        "          ].to_dict('records')\n",
        "        assert len(sample_records) == 1, sample_records\n",
        "        sample_record = sample_records[0]\n",
        "        ci = f'[{record[\"ci_lower\"]:.2f}{record[\"ci_upper\"]:.2f}]'.replace(\n",
        "            '0.', '.'\n",
        "        )\n",
        "        label = f'{record[\"mean\"]:0.3f}\\n{ci}\\nn={sample_record[\"mean\"]:0.0f}'\n",
        "      dim_labels.append(label)\n",
        "    annot_labels.append(dim_labels)\n",
        "  sns.heatmap(\n",
        "      data=sub_df_pivot,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      annot=annot_labels,\n",
        "      fmt='',\n",
        "      ax=ax,\n",
        "      cmap=sns.cubehelix_palette(as_cmap=True, reverse=True),\n",
        "      cbar=plot_cmap,\n",
        "  )\n",
        "  num_core = len([x for x in sub_df_pivot.index if 'Primary' in x])\n",
        "  ax.axhline(y=num_core, color='black', linewidth=5)\n",
        "  ax.axvline(x=num_core, color='black', linewidth=5)\n",
        "  ax.tick_params(axis='x', labelrotation=label_rotation_x)\n",
        "  ax.tick_params(axis='y', labelrotation=label_rotation_y)\n",
        "  title = (\n",
        "      METRIC_TO_LABEL_BINARY[metric_name]\n",
        "      if title_prefix is None\n",
        "      else f'{title_prefix} {METRIC_TO_LABEL_BINARY[metric_name]}'\n",
        "  )\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlabel('')\n",
        "  ax.set_ylabel('')\n",
        "\n",
        "\n",
        "def plot_rater_pair_stats_binary_only_gwet(\n",
        "    sleep_df: pd.DataFrame,\n",
        "    readiness_df: pd.DataFrame,\n",
        "    filename: str | None = None,\n",
        ") -\u003e None:\n",
        "  fig, axes = plt.subplots(\n",
        "      nrows=2,\n",
        "      ncols=2,\n",
        "      figsize=(25, 15),\n",
        "      dpi=300,\n",
        "      sharey=False,\n",
        "      sharex=False,\n",
        "      gridspec_kw={'height_ratios': [1, 20]},\n",
        "  )\n",
        "  gs = axes[0, 0].get_gridspec()\n",
        "  # remove the underlying Axes\n",
        "  for ax in axes[0, :]:\n",
        "    ax.remove()\n",
        "  legend_ax = fig.add_subplot(gs[0, :])\n",
        "  _plot_stats_for_metric_binary(\n",
        "      sleep_df,\n",
        "      'gwet_ac2',\n",
        "      axes[1][0],\n",
        "      plot_cmap=False,\n",
        "      label_rotation_x=45,\n",
        "      label_rotation_y=45,\n",
        "      title_prefix='Sleep:',\n",
        "  )\n",
        "  _plot_stats_for_metric_binary(\n",
        "      readiness_df,\n",
        "      'gwet_ac2',\n",
        "      axes[1][1],\n",
        "      plot_cmap=False,\n",
        "      label_rotation_x=45,\n",
        "      label_rotation_y=45,\n",
        "      title_prefix='Fitness:',\n",
        "  )\n",
        "  fig.colorbar(\n",
        "      axes[1][1].get_children()[0],\n",
        "      cax=legend_ax,\n",
        "      orientation='horizontal',\n",
        "  )\n",
        "  legend_ax.xaxis.set_ticks_position('top')\n",
        "  plt.subplots_adjust(wspace=0.075, hspace=0.075)\n",
        "\n",
        "  # Add a, b, etc. labels to subfigures.\n",
        "  labeled_axes = [*fig.get_axes()]\n",
        "  for i, ax in enumerate(labeled_axes[:-1]):  # Skip the legend.\n",
        "    ax_label = f'{string.ascii_lowercase[i]}'\n",
        "    trans = transforms.ScaledTranslation(-20 / 72, 7 / 72, fig.dpi_scale_trans)\n",
        "    ax.text(\n",
        "        -0.04,\n",
        "        1.0,\n",
        "        ax_label,\n",
        "        transform=ax.transAxes + trans,\n",
        "        fontsize='18',\n",
        "        va='bottom',\n",
        "        weight='bold',\n",
        "    )\n",
        "\n",
        "  plt.tight_layout()\n",
        "  if filename:\n",
        "    fig.savefig(filename, format='pdf', dpi=300)\n",
        "\n",
        "\n",
        "def _prep_df(\n",
        "    base_df: pd.DataFrame,\n",
        "    autoeval_df: pd.DataFrame,\n",
        "    autoeval_rating_col: str,\n",
        "    autoeval_rater: str,\n",
        ") -\u003e Any:\n",
        "  # We append derived keys to the base dataframe.\n",
        "  base_df = base_df.copy()\n",
        "  base_df[DERIVED_COL_KEY] = base_df.apply(\n",
        "      _key_column,\n",
        "      axis=1,\n",
        "  )\n",
        "  base_df[DERIVED_COL_KEY_NO_RATER] = base_df.apply(\n",
        "      _key_no_rater_column,\n",
        "      axis=1,\n",
        "  )\n",
        "\n",
        "  # We subset the AutoEval dataframe to samples from the AutoEval rater the\n",
        "  # model was trained on, overwrite the rater to be the AutoEval model, append\n",
        "  # derived keys, and rename the rating column.\n",
        "  autoeval_df = autoeval_df.copy()\n",
        "  autoeval_df = autoeval_df[autoeval_df.rater == autoeval_rater].reset_index(\n",
        "      drop=True\n",
        "  )\n",
        "  autoeval_df[COL_RATER] = autoeval_rating_col\n",
        "  autoeval_df[DERIVED_COL_KEY] = autoeval_df.apply(\n",
        "      _key_column,\n",
        "      axis=1,\n",
        "  )\n",
        "  autoeval_df[DERIVED_COL_KEY_NO_RATER] = autoeval_df.apply(\n",
        "      _key_no_rater_column,\n",
        "      axis=1,\n",
        "  )\n",
        "  autoeval_df = autoeval_df.drop(columns=[COL_RATING_HUMAN_EXPERT]).rename(\n",
        "      columns={autoeval_rating_col: COL_RATING}\n",
        "  )\n",
        "  df_merged = pd.concat([base_df, autoeval_df])\n",
        "\n",
        "  # Here we build pairs that contain the autorater and/or the rater used to\n",
        "  # train the autorater. We fix ordering here for easier comparison.\n",
        "  rater_pairs_rater = []\n",
        "  rater_pairs_auto = []\n",
        "  rater_pairs_mixed = []\n",
        "  for pair in get_rater_pairs(df_merged):\n",
        "    if (autoeval_rater in pair) and (autoeval_rating_col in pair):\n",
        "      rater_pairs_mixed.append((autoeval_rating_col, autoeval_rater))\n",
        "    elif autoeval_rater in pair:\n",
        "      if pair[0] == autoeval_rater:\n",
        "        rater_pairs_rater.append((pair[1], pair[0]))\n",
        "      else:\n",
        "        rater_pairs_rater.append(pair)\n",
        "    elif autoeval_rating_col in pair:\n",
        "      if pair[0] == autoeval_rating_col:\n",
        "        rater_pairs_auto.append((pair[1], pair[0]))\n",
        "      else:\n",
        "        rater_pairs_auto.append(pair)\n",
        "  rater_pairs = (\n",
        "      sorted(\n",
        "          rater_pairs_rater,\n",
        "          key=lambda x: AUTOEVAL_MODEL_TO_LABEL.get(\n",
        "              x[0], _rater_id_to_label(x[0])\n",
        "          ),\n",
        "      )\n",
        "      + sorted(\n",
        "          rater_pairs_auto,\n",
        "          key=lambda x: AUTOEVAL_MODEL_TO_LABEL.get(\n",
        "              x[0], _rater_id_to_label(x[0])\n",
        "          ),\n",
        "      )\n",
        "      + sorted(\n",
        "          rater_pairs_mixed,\n",
        "          key=lambda x: AUTOEVAL_MODEL_TO_LABEL.get(\n",
        "              x[0], _rater_id_to_label(x[0])\n",
        "          ),\n",
        "      )\n",
        "  )\n",
        "  colors = (\n",
        "      [COLOR_BLUE] * len(rater_pairs_rater)\n",
        "      + [COLOR_GREEN] * len(rater_pairs_auto)\n",
        "      + [COLOR_YELLOW] * len(rater_pairs_mixed)\n",
        "  )\n",
        "\n",
        "  df_pivot = df_merged.pivot(\n",
        "      index=DERIVED_COL_KEY_NO_RATER,\n",
        "      columns=COL_RATER,\n",
        "      values=COL_RATING,\n",
        "  )\n",
        "  return rater_pairs, colors, df_pivot\n",
        "\n",
        "\n",
        "def plot_rater_contingency_autorater_rater_overlap_both(\n",
        "    sleep_df: pd.DataFrame,\n",
        "    sleep_autoeval_df: pd.DataFrame,\n",
        "    sleep_autoeval_rating_col: str,\n",
        "    sleep_autoeval_rater_id: str,\n",
        "    fitness_df: pd.DataFrame,\n",
        "    fitness_autoeval_df: pd.DataFrame,\n",
        "    fitness_autoeval_rating_col: str,\n",
        "    fitness_autoeval_rater_id: str,\n",
        "    filename: str | None = None,\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Plots a contigency plot for the given autorater.\n",
        "\n",
        "  This subsets to only overlaps with the training autorater (i.e., we take\n",
        "  all train rater v.s. other rater pairs and report concord). This is for easier\n",
        "  comparison with the original rater.\n",
        "  \"\"\"\n",
        "  sleep_df = sleep_df.copy()\n",
        "  sleep_autoeval_df = sleep_autoeval_df.copy()\n",
        "  fitness_df = fitness_df.copy()\n",
        "  fitness_autoeval_df = fitness_autoeval_df.copy()\n",
        "\n",
        "  sleep_rater_pairs, sleep_colors, sleep_df_pivot = _prep_df(\n",
        "      sleep_df,\n",
        "      sleep_autoeval_df,\n",
        "      sleep_autoeval_rating_col,\n",
        "      sleep_autoeval_rater_id,\n",
        "  )\n",
        "  fitness_rater_pairs, fitness_colors, fitness_df_pivot = _prep_df(\n",
        "      fitness_df,\n",
        "      fitness_autoeval_df,\n",
        "      fitness_autoeval_rating_col,\n",
        "      fitness_autoeval_rater_id,\n",
        "  )\n",
        "\n",
        "  num_rows = 5\n",
        "  num_cols = 4\n",
        "  fig, axes = plt.subplots(\n",
        "      num_rows,\n",
        "      num_cols,\n",
        "      figsize=(4 * num_cols, 4 * num_rows),\n",
        "      dpi=300,\n",
        "  )\n",
        "  # Handle matplotlib returning varied types/shapes for subplots.\n",
        "  if not isinstance(axes, np.ndarray):\n",
        "    axes = [[axes]]\n",
        "  elif len(axes.shape) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  used_axes = []\n",
        "  for row_offset, df_pivot, colors, rater_pairs in [\n",
        "      (0, sleep_df_pivot, sleep_colors, sleep_rater_pairs),\n",
        "      (\n",
        "          2,\n",
        "          fitness_df_pivot,\n",
        "          fitness_colors,\n",
        "          fitness_rater_pairs,\n",
        "      ),\n",
        "  ]:\n",
        "    num_pairs = len(rater_pairs)\n",
        "    row_indices, col_indices = np.unravel_index(\n",
        "        range(num_pairs + 1 if row_offset == 0 else num_pairs),\n",
        "        (2 if row_offset == 0 else 3, num_cols),\n",
        "        order='C',\n",
        "    )\n",
        "    for n, (rater_a, rater_b) in enumerate(rater_pairs):\n",
        "      ax_n = n\n",
        "      if row_offset == 0 and n \u003e 2:\n",
        "        if n \u003e num_pairs:\n",
        "          continue\n",
        "        ax_n += 1\n",
        "      label_a = AUTOEVAL_MODEL_TO_LABEL.get(\n",
        "          rater_a, _rater_id_to_label(rater_a)\n",
        "      )\n",
        "      label_b = AUTOEVAL_MODEL_TO_LABEL.get(\n",
        "          rater_b, _rater_id_to_label(rater_b)\n",
        "      )\n",
        "      rater_pair_df = (\n",
        "          df_pivot[[\n",
        "              rater_a,\n",
        "              rater_b,\n",
        "          ]]\n",
        "          .dropna()\n",
        "          .reset_index(drop=True)\n",
        "      )\n",
        "      ax = axes[row_indices[ax_n] + row_offset][col_indices[ax_n]]\n",
        "      used_axes.append(ax)\n",
        "      plot_rating_contingency(\n",
        "          rater_pair_df,\n",
        "          col_a=rater_a,\n",
        "          label_a=label_a,\n",
        "          col_b=rater_b,\n",
        "          label_b=label_b,\n",
        "          ax=ax,\n",
        "          color=colors[n],\n",
        "      )\n",
        "\n",
        "  for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "      ax = axes[i, j]\n",
        "      if ax not in used_axes:\n",
        "        ax.set_visible(False)\n",
        "\n",
        "  labeled_axes = [axes[0][0], axes[2][0]]\n",
        "\n",
        "  for i, ax in enumerate(labeled_axes):  # Skip the legend.\n",
        "    ax_label = f'{string.ascii_lowercase[i]}'\n",
        "    trans = transforms.ScaledTranslation(-20 / 72, 7 / 72, fig.dpi_scale_trans)\n",
        "    ax.text(\n",
        "        -0.04,\n",
        "        1.0,\n",
        "        ax_label,\n",
        "        transform=ax.transAxes + trans,\n",
        "        fontsize='18',\n",
        "        va='bottom',\n",
        "        weight='bold',\n",
        "    )\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "  if filename:\n",
        "    fig.savefig(filename, format='pdf', dpi=300)\n",
        "\n",
        "\n",
        "def _plot_pro_subfig(\n",
        "    ax: plt.Axes,\n",
        "    df: pd.DataFrame,\n",
        "    df_prevalence: pd.DataFrame,\n",
        "    metric: str,\n",
        "    models: list[str],\n",
        "    add_significance: bool,\n",
        "    targets: list[str] | None = None,\n",
        "    palette: dict[str, str] | None = None,\n",
        "    fontsize: str = '18',\n",
        ") -\u003e None:\n",
        "  \"\"\"Plot PRO sub-figure.\"\"\"\n",
        "  if targets is None:\n",
        "    targets = PRO_BINARY_TARGETS\n",
        "  if palette is None:\n",
        "    palette = PRO_MODEL_TO_COLOR\n",
        "  num_patches = len(models) * len(targets)\n",
        "  metric_config = PRO_METRIC_TO_CONFIGS[metric]\n",
        "\n",
        "  # Add random guess.\n",
        "  if metric == 'auc':\n",
        "    # Random guess has 0.5 success rate for AUROC.\n",
        "    plt.sca(ax)\n",
        "    plt.axvline(x=0.5, color='black', linestyle='--')\n",
        "  else:\n",
        "    # Add prevalence for AUPRC.\n",
        "    df = pd.concat([df, df_prevalence], ignore_index=True)\n",
        "    new_models = list(df_prevalence[PRO_COL_MODEL].unique())\n",
        "    models = models + new_models\n",
        "    num_patches += len(new_models) * len(targets)\n",
        "\n",
        "  # Add main plot.\n",
        "  sns.barplot(\n",
        "      y=PRO_COL_TARGET,\n",
        "      x='mean',\n",
        "      hue=PRO_COL_MODEL,\n",
        "      data=df,\n",
        "      n_boot=100,\n",
        "      palette=palette,\n",
        "      order=targets,\n",
        "      hue_order=models,\n",
        "      orient='h',\n",
        "      ax=ax,\n",
        "  )\n",
        "\n",
        "  # Add error bar.\n",
        "  err_lower = (df['mean'] - df['ci_lower']).values.flatten()\n",
        "  err_upper = (df['ci_upper'] - df['mean']).values.flatten()\n",
        "  err = (err_lower, err_upper)\n",
        "  y = [\n",
        "      patch.get_xy()[1] + patch.get_height() / 2.0\n",
        "      for patch in ax.patches[:num_patches]\n",
        "  ]\n",
        "  _ = ax.errorbar(x=df['mean'], y=y, xerr=err, fmt='o', color='black')\n",
        "\n",
        "  # Add significant mark.\n",
        "  if add_significance:\n",
        "    indices_non_sig = metric_config['non_significant_indices']\n",
        "    indices_significant_improve = [\n",
        "        i for i in range(len(targets)) if i not in indices_non_sig\n",
        "    ]\n",
        "    for i in indices_significant_improve:\n",
        "      label_position = i\n",
        "      y_coords = get_bar_mid_y_coords([models[0], models[-1]], label_position, ax)\n",
        "      add_significance_y(\n",
        "          ax, y_coords[0], y_coords[1], *metric_config['significant_loc'], '*'\n",
        "      )\n",
        "\n",
        "  # Adjust legend, title, xlabel, and ylabel.\n",
        "  ax.legend(\n",
        "      loc=metric_config['legend_loc'],\n",
        "      facecolor='white',\n",
        "      edgecolor='white',\n",
        "      framealpha=1,\n",
        "      handlelength=0.7,\n",
        "      borderpad=0.3,\n",
        "      fontsize=fontsize\n",
        "  )\n",
        "  ax.set_title('')\n",
        "  ax.set_xlabel(metric_config['xlabel'], fontsize=fontsize)\n",
        "  ax.set_ylabel('')\n",
        "  ax.tick_params(axis='x', labelsize=fontsize)\n",
        "  ax.tick_params(axis='y', labelsize=fontsize)\n",
        "\n",
        "\n",
        "def plot_pro_fig(\n",
        "    models: list[str],\n",
        "    df_pro: pd.DataFrame,\n",
        "    df_pro_prevalence: pd.DataFrame,\n",
        "    savefig_filepath: str,\n",
        "    add_significance: bool = False,\n",
        "    figsize: list[int] = [16, 10],\n",
        "    subfig_idx_offset: int = 0,\n",
        "    subfig_idx_pos: list[list[float]] = [[0.1, 0.9], [0.6, 0.9]],\n",
        "    fontsize: str = '12',\n",
        ") -\u003e None:\n",
        "  \"\"\"Plot prediction part of the PRO main figure, i.e., figure 3 c,d.\"\"\"\n",
        "  fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "  for axes_idx, metric in enumerate(PRO_METRIC_TO_CONFIGS):\n",
        "    _plot_pro_subfig(\n",
        "        ax=axes[axes_idx],\n",
        "        df=df_pro[df_pro['metric_name'] == metric].reset_index().copy(),\n",
        "        df_prevalence=df_pro_prevalence,\n",
        "        metric=metric,\n",
        "        models=models,\n",
        "        add_significance=add_significance,\n",
        "        fontsize=fontsize,\n",
        "    )\n",
        "  # Add a,b,c,d like labels to subfigures.\n",
        "  # Avoid adding label to heatmap color bar.\n",
        "  labeled_axes = [\n",
        "      ax for ax in [*fig.get_axes()] if ax.get_label() != '\u003ccolorbar\u003e'\n",
        "  ]\n",
        "  for i, ax in enumerate(labeled_axes):\n",
        "    ax_label = string.ascii_lowercase[i + subfig_idx_offset]\n",
        "    trans = transforms.ScaledTranslation(-20 / 72, 7 / 72, fig.dpi_scale_trans)\n",
        "    ax.text(\n",
        "        subfig_idx_pos[i][0],\n",
        "        subfig_idx_pos[i][1],\n",
        "        ax_label,\n",
        "        transform=fig.transFigure + trans,\n",
        "        fontsize='16',\n",
        "        va='bottom',\n",
        "        weight='bold',\n",
        "    )\n",
        "  plt.tight_layout()\n",
        "  fig.savefig(savefig_filepath, format='pdf', dpi=300)\n",
        "  plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "i8g3OqzG5ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "## Load human expert and AutoEval ratings"
      ]
    },
    {
      "metadata": {
        "id": "r3s4_mg75ZCM"
      },
      "cell_type": "code",
      "source": [
        "# Human expert ratings for internal models.\n",
        "g_sleep_df = load_ratings_df(\n",
        "    Vertical.SLEEP,\n",
        "    RatingsSource.HUMAN_EXPERT,\n",
        ")\n",
        "g_fitness_df = load_ratings_df(\n",
        "    Vertical.FITNESS,\n",
        "    RatingsSource.HUMAN_EXPERT,\n",
        ")\n",
        "\n",
        "# AutoEval ratings for internal and external models.\n",
        "g_sleep_external_df = load_ratings_df(\n",
        "    Vertical.SLEEP,\n",
        "    RatingsSource.AUTOEVAL_EXTERNAL,\n",
        ")\n",
        "g_fitness_external_df = load_ratings_df(\n",
        "    Vertical.FITNESS,\n",
        "    RatingsSource.AUTOEVAL_EXTERNAL,\n",
        ")\n",
        "\n",
        "# AutoEval ratings for subsampled PH-LLM models.\n",
        "g_sleep_subsample_df = load_ratings_df(\n",
        "    Vertical.SLEEP,\n",
        "    RatingsSource.AUTOEVAL_SUBSAMPLE,\n",
        ")\n",
        "g_fitness_subsample_df = load_ratings_df(\n",
        "    Vertical.FITNESS,\n",
        "    RatingsSource.AUTOEVAL_SUBSAMPLE,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Fl_GfxsS5ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot figures"
      ]
    },
    {
      "metadata": {
        "id": "It94sUAN5ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "### Fig. 2\n",
        "\n",
        "**Long-form case study evaluation and performance.** [...] c,d, Mean ratings\n",
        "given by experts for the case study subsections across the sleep (c) and fitness\n",
        "(d) domains. Error bars represent 95% confidence intervals.  indicates a\n",
        "statistically significant difference between two response types after multiple\n",
        "hypothesis testing correction. REM: Rapid Eye Movement, HRV RMSSD: Heart Rate\n",
        "Variability Root Mean Square of Successive Differences, ACWR: Acute:Chronic\n",
        "Workload Ratio."
      ]
    },
    {
      "metadata": {
        "id": "Li6UGavm5ZCM"
      },
      "cell_type": "code",
      "source": [
        "# Note: Since we do not include Fig. 2's case study examples as the first two\n",
        "# subfigures here, the subplots in the figure below are annotated \"a\" and \"b\".\n",
        "plot_case_study_main_fig(\n",
        "    sleep_df=g_sleep_df,\n",
        "    fitness_df=g_fitness_df,\n",
        "    sleep_title=Vertical.SLEEP.value.capitalize(),\n",
        "    fitness_title=Vertical.FITNESS.value.capitalize(),\n",
        "    sleep_rating_col=COL_RATING,\n",
        "    fitness_rating_col=COL_RATING,\n",
        "    hue_order=CONVERSATION_SOURCE_ORDER_MAIN,\n",
        "    label_subplots=True,\n",
        "    savefig_filepath='figure_2_cd.pdf',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d9u4b8kw8bJt"
      },
      "cell_type": "markdown",
      "source": [
        "### Fig. 3\n",
        "**Prediction of patient-reported outcomes by PH-LLM.** [...] c, Area under the receiver operating characteristic curve (AUROC) for the performance of PH-LLM with adapter,\n",
        "zero-shot, and few-shot prompting approaches when predicting binary outcomes derived from survey responses. The\n",
        "dotted vertical line denotes the AUROC of the random predictor. Outcomes for which the performance of PH-LLM\n",
        "with adapter approach is significantly better than both zero- and few-shot are annotated with *. d, Area under the\n",
        "precision-recall curve (AUPRC) for the performance of PH-LLM with adapter, zero-shot, and few-shot prompting\n",
        "approaches when predicting binary outcomes derived from survey responses. Outcome-specific prevalence bars are\n",
        "added to show the AUPRC of the random predictor. Survey response names are mapped to their corresponding questions\n",
        "in Supplementary Tables 39 and 40. SI, sleep impairment. Error bars represent 95% confidence intervals."
      ]
    },
    {
      "metadata": {
        "id": "N6yed7MC8dTL"
      },
      "cell_type": "code",
      "source": [
        "# Load PRO data.\n",
        "models = ['PH-LLM w/ Adapter', 'PH-LLM Few-shot', 'PH-LLM Zero-shot']\n",
        "df_pro, df_pro_prevalence = load_pro_df(models)\n",
        "# Plot.\n",
        "plot_pro_fig(\n",
        "    models,\n",
        "    df_pro,\n",
        "    df_pro_prevalence,\n",
        "    savefig_filepath='figure_3_cd.pdf',\n",
        "    add_significance=True,\n",
        "    subfig_idx_offset=2,  # This changes indices from a,b to c,d.\n",
        "    fontsize='12',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "B2XgLid05ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 3\n",
        "\n",
        "**Pairwise Gwet's AC2 measuring inter-rater reliability between primary and\n",
        "secondary raters.** Metrics were computed using all ratings for each principle\n",
        "and section across case studies rated by more than one rater in the sleep (a)\n",
        "and fitness (b) domains. The number of overlapping ratings is denoted by n. Mean\n",
        "metrics and 95% confidence intervals derived from 1,000 bootstrapping iterations\n",
        "are reported for each pair."
      ]
    },
    {
      "metadata": {
        "id": "OGjz0Ryk5ZCM"
      },
      "cell_type": "code",
      "source": [
        "g_sleep_bootstrap_metrics_df = bootstrap_rater_pairs_binary(g_sleep_df)\n",
        "g_fitness_bootstrap_metrics_df = bootstrap_rater_pairs_binary(g_fitness_df)\n",
        "plot_rater_pair_stats_binary_only_gwet(\n",
        "    g_sleep_bootstrap_metrics_df,\n",
        "    g_fitness_bootstrap_metrics_df,\n",
        "    filename='extended_data_figure_3.pdf',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YuCutrI75ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 4\n",
        "\n",
        "**Contingency tables showing pairwise rating agreement between raters.** Counts\n",
        "are aggregated across all case studies, sections, and principles for each case\n",
        "study for which multiple ratings are available in the sleep (a) and fitness (b)\n",
        "domains. Blue, primary versus primary raters. Green, primary versus secondary\n",
        "raters. Yellow, secondary versus secondary raters."
      ]
    },
    {
      "metadata": {
        "id": "Pq_qHGkO5ZCM"
      },
      "cell_type": "code",
      "source": [
        "plot_rater_contingency_all_both_verticals(\n",
        "    g_sleep_df,\n",
        "    g_fitness_df,\n",
        "    filename='extended_data_figure_4.pdf',\n",
        "    rater_types_a=[RaterType.PRIMARY, RaterType.PRIMARY, RaterType.SECONDARY],\n",
        "    rater_types_b=[RaterType.PRIMARY, RaterType.SECONDARY, RaterType.SECONDARY],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e6t00Fgy5ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 5\n",
        "\n",
        "**Sleep and fitness case study human evaluation results by principle.** Mean\n",
        "ratings given by experts for different case study evaluation principles across\n",
        "all sections in the sleep (a) and fitness (b) domains. The principles are\n",
        "ordered according to the rubric presented in Supplementary Table 9. \n",
        "indicates a statistically significant difference between two response types\n",
        "after multiple hypothesis testing correction. Error bars represent 95%\n",
        "confidence intervals."
      ]
    },
    {
      "metadata": {
        "id": "IZxDiRH-5ZCM"
      },
      "cell_type": "code",
      "source": [
        "plot_by_principle_all(\n",
        "    sleep_df=g_sleep_df,\n",
        "    fitness_df=g_fitness_df,\n",
        "    sleep_title=Vertical.SLEEP.value.capitalize(),\n",
        "    fitness_title=Vertical.FITNESS.value.capitalize(),\n",
        "    order=PRINCIPLE_ORDER,\n",
        "    # Note: We reverse the order since vertical bar plots are bottom-to-top.\n",
        "    hue_order=CONVERSATION_SOURCE_ORDER_MAIN[::-1],\n",
        "    other_font_size=10,\n",
        "    add_stat_sig_annot=True,\n",
        "    label_subplots=True,\n",
        "    savefig_filepath='extended_data_figure_5.pdf',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Xxb6PB-F5ZCM"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 6\n",
        "\n",
        "**Contingency tables showing pairwise rating agreement between our best\n",
        "AutoRaters, their corresponding expert raters, and other experts.** Counts are\n",
        "aggregated across all case studies, sections, and principles for each case study\n",
        "for which at least one rating from the AutoEval training rater is available in\n",
        "the sleep (a) and fitness (b) domains. Blue, the primary expert rater versus\n",
        "other raters. Green, the AutoEval model trained on primary expert ratings versus\n",
        "other raters. Yellow, the primary expert rater versus the corresponding AutoEval\n",
        "model."
      ]
    },
    {
      "metadata": {
        "id": "glsE2XQ1TroL"
      },
      "cell_type": "code",
      "source": [
        "plot_rater_contingency_autorater_rater_overlap_both(\n",
        "    sleep_df=g_sleep_df,\n",
        "    sleep_autoeval_df=g_sleep_external_df,\n",
        "    sleep_autoeval_rating_col=COL_RATING_AUTOEVAL_SLEEP,\n",
        "    sleep_autoeval_rater_id='sleep_primary_c',\n",
        "    fitness_df=g_fitness_df,\n",
        "    fitness_autoeval_df=g_fitness_external_df,\n",
        "    fitness_autoeval_rating_col=COL_RATING_AUTOEVAL_FITNESS,\n",
        "    fitness_autoeval_rater_id='fitness_primary_c',\n",
        "    filename='extended_data_figure_6.pdf',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "FuoRKuUDqRuK"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 7\n",
        "\n",
        "**Automatic evaluation of coaching recommendations across PH-LLM and baseline\n",
        "models.** Mean ratings were generated using our best AutoEval models in the\n",
        "sleep (a) and fitness (b) domains. Within each section, a * denotes the\n",
        "highest-rated model and all models not statistically significantly different\n",
        "from that model after multiple hypothesis testing correction. Error bars\n",
        "represent 95% confidence intervals."
      ]
    },
    {
      "metadata": {
        "id": "RnrGYtZ_DD54"
      },
      "cell_type": "code",
      "source": [
        "plot_case_study_main_fig(\n",
        "    sleep_df=g_sleep_external_df,\n",
        "    fitness_df=g_fitness_external_df,\n",
        "    sleep_title=Vertical.SLEEP.value.capitalize(),\n",
        "    fitness_title=Vertical.FITNESS.value.capitalize(),\n",
        "    sleep_rating_col=COL_RATING_AUTOEVAL_SLEEP,\n",
        "    fitness_rating_col=COL_RATING_AUTOEVAL_FITNESS,\n",
        "    hue_order=CONVERSATION_SOURCE_ORDER_EXTERNAL,\n",
        "    label_subplots=True,\n",
        "    pairwise_statsig=False,\n",
        "    savefig_filepath='extended_data_figure_7.pdf',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "0LpYRYbiqgzR"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 8\n",
        "\n",
        "**Effect of fine-tuning data scale on model performance in coaching\n",
        "recommendations.** Ratings are obtained via the best AutoEval models for the\n",
        "holdout case study subsections in the sleep (a) and fitness (b) domains.\n",
        "PH-LLM denotes standard performance while Subsampled 25% and Subsampled\n",
        "50% denote responses from models trained on 25% and 50% of the training\n",
        "dataset, respectively. Gemini Ultra 1.0 denotes untuned baseline performance\n",
        "(i.e., trained on 0% of the training dataset). Within each section, a *\n",
        "denotes the highest-rated model and all models not statistically significantly\n",
        "different from that model after multiple hypothesis testing correction. Error\n",
        "bars represent 95% confidence intervals."
      ]
    },
    {
      "metadata": {
        "id": "dKN5PMGbpWqq"
      },
      "cell_type": "code",
      "source": [
        "plot_case_study_main_fig(\n",
        "    sleep_df=g_sleep_subsample_df,\n",
        "    fitness_df=g_fitness_subsample_df,\n",
        "    sleep_title=Vertical.SLEEP.value.capitalize(),\n",
        "    fitness_title=Vertical.FITNESS.value.capitalize(),\n",
        "    sleep_rating_col=COL_RATING_AUTOEVAL_SLEEP,\n",
        "    fitness_rating_col=COL_RATING_AUTOEVAL_FITNESS,\n",
        "    hue_order=CONVERSATION_SOURCE_ORDER_SUBSAMPLE,\n",
        "    label_subplots=True,\n",
        "    pairwise_statsig=False,\n",
        "    savefig_filepath='extended_data_figure_8.pdf',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "g3ToGjPbrfwF"
      },
      "cell_type": "markdown",
      "source": [
        "### Extended Data Fig. 9\n",
        "\n",
        "**Performance of PH-LLM and traditional ML models on patient-reported outcomes\n",
        "prediction.** We compared the ability of PH-LLM with and without a multimodal adapter, logistic regression, and a\n",
        "convolutional neural network (CNN) to infer subjective patient-reported outcomes. a, Area under the receiver operating\n",
        "characteristic curve (AUROC). b, Area under the precision-recall curve (AUPRC). Error bars represent 95% confidence\n",
        "intervals. The CNN underperforms logistic regression, likely due to the limited size of the dataset."
      ]
    },
    {
      "metadata": {
        "id": "pnKxf378rsz0"
      },
      "cell_type": "code",
      "source": [
        "# Load PRO data.\n",
        "models = ['PH-LLM w/ Adapter', 'PH-LLM Few-shot', 'PH-LLM Zero-shot', 'LogReg', 'CNN']\n",
        "df_pro, df_pro_prevalence = load_pro_df(models)\n",
        "# Plot.\n",
        "plot_pro_fig(\n",
        "    models,\n",
        "    df_pro,\n",
        "    df_pro_prevalence,\n",
        "    savefig_filepath='extended_data_figure_9.pdf',\n",
        "    figsize=[22, 22],\n",
        "    subfig_idx_pos=[[0.05, 0.92], [0.55, 0.92]],\n",
        "    fontsize='16',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
